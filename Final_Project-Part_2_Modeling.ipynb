{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0272e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aburshtein/anaconda3/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations # not mandatory\n",
    "import imutils\n",
    "from imutils import build_montages, paths\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost\n",
    "from xgboost import cv, XGBClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ded158",
   "metadata": {},
   "source": [
    "## Cleaning and Arranging the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a73e32",
   "metadata": {},
   "source": [
    "### Define useful functions for cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a002848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses(string):\n",
    "    pattern = r'\\([^()]*\\)'  # Matches \"(...)\" pattern\n",
    "    while re.search(pattern, string):\n",
    "        string = re.sub(pattern, '', string)\n",
    "    return string.strip()\n",
    "\n",
    "def format_ingredients(string):\n",
    "    string = string.replace('.', '')  # Remove periods\n",
    "    string = re.sub(r'\\s*,\\s*', ',', string)  # Remove spaces after commas\n",
    "    ingredients_list = string.split(',')  # Split by comma\n",
    "    formatted_ingredients = [ingredient.strip() for ingredient in ingredients_list]  # Remove leading/trailing spaces for each ingredient\n",
    "    return ', '.join(formatted_ingredients)  # Join formatted ingredients with commas\n",
    "\n",
    "def clean_text_values(df):\n",
    "    text_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    for column in text_columns:\n",
    "        if column == 'category':\n",
    "            continue\n",
    "        df[column].fillna('NA', inplace=True)\n",
    "        df[column] = df[column].map(str.lower)\n",
    "        if column == 'ingredients':\n",
    "            df[column] = df[column].apply(remove_parentheses)\n",
    "            df[column] = df[column].apply(format_ingredients)\n",
    "        if column == 'household_serving_fulltext':\n",
    "            df[column] = df[column].map(lambda x: re.sub('[^a-z]+', '', x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81773c97-2259-43e0-8227-faaf84b5424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca7df747",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208e40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_train = pd.read_csv('data/food_train.csv')\n",
    "food_test = pd.read_csv('data/food_test.csv')\n",
    "food_nutrients = pd.read_csv('data/food_nutrients.csv')\n",
    "nutrients_names = pd.read_csv('data/nutrients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f7d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_values(food_train)\n",
    "nutrients = pd.merge(food_nutrients, nutrients_names, how='left',on='nutrient_id')\n",
    "pivoted_nutrients = pd.pivot_table(nutrients, values='amount', index='idx', columns='name')\n",
    "data = pd.merge(food_train, pivoted_nutrients, how='left', on='idx')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea9ac8",
   "metadata": {},
   "source": [
    "### Remove columns with > 0.8 nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59fe0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = data.columns[data.isnull().mean() > 0.8]\n",
    "data = data.drop(columns=cols_to_remove)\n",
    "\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae706ab0",
   "metadata": {},
   "source": [
    "### Splite the data\n",
    "\n",
    "we'll split our df to train and test sets. Then, we'll split the train set into 2 data sets. One for feature engineering and the other for model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b407f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,df.columns != 'category']\n",
    "y = df.loc[:,['category']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_fe, X_mt, y_fe, y_mt = train_test_split(X_train, y_train, test_size=0.5, random_state=42, stratify=y_train)\n",
    "X_fe_train, X_fe_test, y_fe_train, y_fe_test = train_test_split(X_fe, y_fe, test_size=0.2, random_state=42)\n",
    "X_mt_train, X_mt_test, y_mt_train, y_mt_test = train_test_split(X_mt, y_mt, test_size=0.2, random_state=42)\n",
    "\n",
    "X_y_train = pd.merge(X_train, y_train, how=\"left\",left_index=True, right_index=True)\n",
    "X_y_train['idx'] = X_y_train['idx'].map(lambda x: str(x))\n",
    "X_y_test = pd.merge(X_test, y_test, how=\"left\",left_index=True, right_index=True)\n",
    "X_y_test['idx'] = X_y_test['idx'].map(lambda x: str(x))\n",
    "\n",
    "#### #### concat for eda\n",
    "X_fe_w_cat = pd.merge(X_fe_train, y_fe_train, how=\"left\",left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ef73e",
   "metadata": {},
   "source": [
    "### Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff4fa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDY = 'candy'\n",
    "COOKIES = 'cookies_biscuits'\n",
    "CAKES = 'cakes_cupcakes_snack_cakes'\n",
    "CHIPS_PRETZELS = 'chips_pretzels_snacks'\n",
    "CHOCOLATE = 'chocolate'\n",
    "POPCORN_PEANUTS = 'popcorn_peanuts_seeds_related_snacks'\n",
    "\n",
    "CATEGORIES = [CANDY, COOKIES, CAKES, CHIPS_PRETZELS, CHOCOLATE, POPCORN_PEANUTS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51b264",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f745de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_name(df, col_name):\n",
    "    df[col_name] = df[col_name].map(lambda x: str(x).translate(str.maketrans('', '', string.punctuation)))\n",
    "    return(df)\n",
    "\n",
    "def data_by_category(df, category):\n",
    "    return df[df['category'] == category]\n",
    "\n",
    "def get_most_popular_words(column):\n",
    "    text = ', '.join(column.values)\n",
    "    tokens = [word.strip() for word in text.split(',')]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    word_counts = Counter(tokens)\n",
    "    return word_counts\n",
    "\n",
    "def select_top_words(row, dict_words, column):\n",
    "    desc = row[column]\n",
    "    for category, words in dict_words.items():\n",
    "        for word in dict_words[category]:\n",
    "            if word in desc:\n",
    "                name = f\"{column}_{category}_{word}\"\n",
    "                return name\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8b2e",
   "metadata": {},
   "source": [
    "# Part 1 - Feature Engineering\n",
    "\n",
    "### Top 15 words in each column in the data per category\n",
    "We have found the most 15 common words in each column: 'brand', 'description', 'ingredients' and 'household' per category. That will help us understand which words are represent and imply each category.\n",
    "\n",
    "You can see all the data analysis we performed in the \"Final_Project-Part_2-Words_Selection\" notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c02e3",
   "metadata": {},
   "source": [
    "# 'brand' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9af844",
   "metadata": {},
   "source": [
    "We will look on top 15 common brands over each category. After that we will change the brand column and make sure that just the selected top 15 (from each category) brands will stay the same and all the other will sign as unbranded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5369cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_x_brands_by_category(df, category, x):\n",
    "    df_by_category = df[df['category'] == category]\n",
    "    return df_by_category.groupby('brand').size().sort_values(ascending=False).head(x)\n",
    "\n",
    "def select_only_the_top_brand(x):\n",
    "    df = X_y_train\n",
    "    better_name(df, 'brand')\n",
    "    top_brand_dict={'candy': top_x_brands_by_category(df, CANDY, x).index.tolist(),\n",
    "                   'cookies': top_x_brands_by_category(df, COOKIES, x).index.tolist(),\n",
    "                   'cakes': top_x_brands_by_category(df, CAKES, x).index.tolist(),\n",
    "                   'chips_pretzels': top_x_brands_by_category(df, CHIPS_PRETZELS, x).index.tolist(),\n",
    "                   'chocolate': top_x_brands_by_category(df, CHOCOLATE, x).index.tolist(),\n",
    "                   'popcorn_peanuts': top_x_brands_by_category(df, POPCORN_PEANUTS, x).index.tolist()}\n",
    "    \n",
    "    return top_brand_dict\n",
    "\n",
    "def get_top_brands(df, x):\n",
    "    top_brand_dict = select_only_the_top_brand(x)\n",
    "    df['brand'].fillna('unbranded',inplace=True)\n",
    "    brands_list = []\n",
    "    for key, value in top_brand_dict.items():\n",
    "        if 'not a branded item' in value:\n",
    "            value.remove('not a branded item')\n",
    "        brands_list.extend(value)  \n",
    "    df['brand'] = df['brand'].map(lambda x: x if x in brands_list else 'unbranded')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d02ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in CATEGORIES:\n",
    "#    common_words_per_category_plot(category, 'brand', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f542340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common_words_per_column('brand', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2aca47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>serving_size_unit</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>Calcium, Ca</th>\n",
       "      <th>Carbohydrate, by difference</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>popcorn_peanuts_ahold usa inc</th>\n",
       "      <th>popcorn_peanuts_john b sanfilippo  son inc</th>\n",
       "      <th>popcorn_peanuts_american importing co inc</th>\n",
       "      <th>popcorn_peanuts_giant eagle inc</th>\n",
       "      <th>popcorn_peanuts_the kroger co</th>\n",
       "      <th>popcorn_peanuts_supervalu inc</th>\n",
       "      <th>popcorn_peanuts_tops markets llc</th>\n",
       "      <th>popcorn_peanuts_kar nut products company</th>\n",
       "      <th>popcorn_peanuts_safeway inc</th>\n",
       "      <th>popcorn_peanuts_whole foods market inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>266</td>\n",
       "      <td>brand_candy_russell stover candies inc</td>\n",
       "      <td>russell stover, nut cluster assortment</td>\n",
       "      <td>consist of milk chocolate1, sugar, whole milk,...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>150.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>33073</td>\n",
       "      <td>None</td>\n",
       "      <td>candy corn</td>\n",
       "      <td>sugar, corn syrup, confectioners glaze, honey,...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>580</td>\n",
       "      <td>None</td>\n",
       "      <td>amande milk chocolate</td>\n",
       "      <td>milk chocolate, almonds, salt and natural color</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>22932</td>\n",
       "      <td>None</td>\n",
       "      <td>pumpkin face taffy</td>\n",
       "      <td>corn syrup, sugar coconut oil, egg whites, sal...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>5968</td>\n",
       "      <td>None</td>\n",
       "      <td>megatoys, happy easter basket, salt water taff...</td>\n",
       "      <td>corn syrup, sugar, palm oil, citric acid, mono...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                   brand  \\\n",
       "241      266  brand_candy_russell stover candies inc   \n",
       "29772  33073                                    None   \n",
       "516      580                                    None   \n",
       "20659  22932                                    None   \n",
       "5350    5968                                    None   \n",
       "\n",
       "                                             description  \\\n",
       "241               russell stover, nut cluster assortment   \n",
       "29772                                         candy corn   \n",
       "516                                amande milk chocolate   \n",
       "20659                                 pumpkin face taffy   \n",
       "5350   megatoys, happy easter basket, salt water taff...   \n",
       "\n",
       "                                             ingredients  serving_size  \\\n",
       "241    consist of milk chocolate1, sugar, whole milk,...          40.0   \n",
       "29772  sugar, corn syrup, confectioners glaze, honey,...          30.0   \n",
       "516      milk chocolate, almonds, salt and natural color          30.0   \n",
       "20659  corn syrup, sugar coconut oil, egg whites, sal...          42.0   \n",
       "5350   corn syrup, sugar, palm oil, citric acid, mono...          43.0   \n",
       "\n",
       "      serving_size_unit household_serving_fulltext  Calcium, Ca  \\\n",
       "241                   g                     pieces        150.0   \n",
       "29772                 g                        grm          0.0   \n",
       "516                   g                        grm        200.0   \n",
       "20659                 g                     pieces          0.0   \n",
       "5350                  g                     pieces          NaN   \n",
       "\n",
       "       Carbohydrate, by difference  Cholesterol  ...  \\\n",
       "241                          45.00         12.0  ...   \n",
       "29772                        96.67          0.0  ...   \n",
       "516                          50.00         23.0  ...   \n",
       "20659                        90.48          0.0  ...   \n",
       "5350                         86.05          0.0  ...   \n",
       "\n",
       "       popcorn_peanuts_ahold usa inc  \\\n",
       "241                                0   \n",
       "29772                              0   \n",
       "516                                0   \n",
       "20659                              0   \n",
       "5350                               0   \n",
       "\n",
       "       popcorn_peanuts_john b sanfilippo  son inc  \\\n",
       "241                                             0   \n",
       "29772                                           0   \n",
       "516                                             0   \n",
       "20659                                           0   \n",
       "5350                                            0   \n",
       "\n",
       "       popcorn_peanuts_american importing co inc  \\\n",
       "241                                            0   \n",
       "29772                                          0   \n",
       "516                                            0   \n",
       "20659                                          0   \n",
       "5350                                           0   \n",
       "\n",
       "       popcorn_peanuts_giant eagle inc  popcorn_peanuts_the kroger co  \\\n",
       "241                                  0                              0   \n",
       "29772                                0                              0   \n",
       "516                                  0                              0   \n",
       "20659                                0                              0   \n",
       "5350                                 0                              0   \n",
       "\n",
       "       popcorn_peanuts_supervalu inc  popcorn_peanuts_tops markets llc  \\\n",
       "241                                0                                 0   \n",
       "29772                              0                                 0   \n",
       "516                                0                                 0   \n",
       "20659                              0                                 0   \n",
       "5350                               0                                 0   \n",
       "\n",
       "       popcorn_peanuts_kar nut products company  popcorn_peanuts_safeway inc  \\\n",
       "241                                           0                            0   \n",
       "29772                                         0                            0   \n",
       "516                                           0                            0   \n",
       "20659                                         0                            0   \n",
       "5350                                          0                            0   \n",
       "\n",
       "       popcorn_peanuts_whole foods market inc  \n",
       "241                                         0  \n",
       "29772                                       0  \n",
       "516                                         0  \n",
       "20659                                       0  \n",
       "5350                                        0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_brand(df):\n",
    "    brand_words = {\n",
    "    \"candy\": [\"ferrara candy company\", \"frankford candy llc\", \"ross acquisition inc\", \"mars chocolate north america llc\",\n",
    "              \"walgreens co\", \"russell stover candies inc\", \"just born inc\", \"sunmark\", \"jelly belly candy company\",\n",
    "              \"holiday candy corp inc\", \"maud borup inc\", \"tops markets llc\", \"weis markets inc\", \"topco associates inc\",\n",
    "              \"tootsie roll industries inc\", \"wm wrigley jr company\", \"reeses\", \"ahold usa inc\"],\n",
    "    \"cookies\": [\"nabisco biscuit company\", \"target stores\", \"keebler company\", \"the kroger co\", \"lofthouse foods\",\n",
    "                \"topco associates inc\", \"meijer inc\", \"ahold usa inc\", \"hyvee inc\", \"safeway inc\", \"walgreens co\",\n",
    "                \"mckee foods corporation\", \"wegmans food markets inc\", \"kingston marketing co\", \"too good gourmet inc\",\n",
    "                \"d f stauffer biscuit co inc\", \"abimar foods inc\",],\n",
    "    \"cakes\": [\"mckee foods corporation\", \"bimbo bakeries usa inc\", \"tasty baking company\", \"the kroger co\", \n",
    "              \"hostess brands llc\", \"sweet ps bake shop\", \"meijer inc\", \"dawn food products inc\", \"flowers foods inc\",\n",
    "              \"rich products corporation\", \"twobite\", \"schnuck markets inc\", \"fresh  easy\", \"dierbergs markets inc\",\n",
    "              \"maplehurst bakeries llc\", \"rocky mountain pies\", \"the fathers table llc\"],\n",
    "    \"chips_pretzels\": [\"utz quality foods inc\", \"the hain celestial group inc\", \"topco associates inc\", \"herr foods inc\",\n",
    "                       \"meijer inc\", \"diamond foods inc\", \"hyvee inc\", \"target stores\", \"walmart stores inc\", \"snyderslance inc\",\n",
    "                       \"inventure foods inc\", \"giant eagle inc\", \"good health natural products inc\", \"the kroger co\",\n",
    "                       \"whole foods market inc\", \"old dutch foods inc\", \"jays foods inc\", \"cape cod potato chips inc\",\n",
    "                       \"wise foods inc\", \"ahold usa inc\"],\n",
    "    \"chocolate\": [\"lindt  sprungli schweiz ag\", \"russell stover candies inc\", \"mars chocolate north america llc\",\n",
    "                  \"ghirardelli chocolate company\", \"godiva chocolatier inc\", \"ghirardelli\", \"fannie may confections inc\",\n",
    "                  \"moonstruck chocolate co\", \"ross acquisition inc\", \"walgreens co\", \"rm palmer co\", \"frankford candy llc\",\n",
    "                  \"theo chocolate inc\", \"hammonds candies since 1920 llc\", \"green  blacks\", \"alter eco americas inc\",\n",
    "                  \"demets candy company\", \"whitmans candies inc\", \"european chocolate ltd\"],\n",
    "    \"popcorn_peanuts\": [\"meijer inc\", \"target stores\", \"nabisco food company\", \"cvs pharmacy inc\", \"walgreens co\",\n",
    "                        \"hyvee inc\", \"topco associates inc\", \"diamond foods inc\", \"ahold usa inc\", \"john b sanfilippo  son inc\",\n",
    "                        \"american importing co inc\", \"giant eagle inc\", \"the kroger co\", \"supervalu inc\", \"tops markets llc\",\n",
    "                        \"kar nut products company\", \"safeway inc\", \"whole foods market inc\"],\n",
    "}\n",
    "\n",
    "    num_rows = len(df)\n",
    "    # Convert the dictionary values to sets to ensure uniqueness\n",
    "    data_dict = {key: set(values) for key, values in brand_words.items()}\n",
    "    # Create a new dictionary with the column names as keys and zeros as values\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in brand_words.items() for word in words}\n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    # Concatenate the existing DataFrame with the new DataFrame\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in brand_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['brand'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['brand'] = result_df.apply(lambda row: select_top_words(row, brand_words, 'brand'), axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "extract_brand(X_fe_w_cat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da0519",
   "metadata": {},
   "source": [
    "# 'description' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a4b0",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack description, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb073965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for category in CATEGORIES:\n",
    "#    common_words_per_category_plot(category, 'description', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f91a2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_common_words_per_column('description', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3504a3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>serving_size_unit</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>Calcium, Ca</th>\n",
       "      <th>Carbohydrate, by difference</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_almond</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_almonds</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_trail mix</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_peanuts</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_mixed nuts</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_dry roasted peanuts</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_cashews</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_kernel</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_shell</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_pecan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>266</td>\n",
       "      <td>russell stover candies inc.</td>\n",
       "      <td>description_chocolate_russell stover</td>\n",
       "      <td>consist of milk chocolate1, sugar, whole milk,...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>150.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>33073</td>\n",
       "      <td>just goodies</td>\n",
       "      <td>description_candy_candy</td>\n",
       "      <td>sugar, corn syrup, confectioners glaze, honey,...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>580</td>\n",
       "      <td>sulpice chocolat</td>\n",
       "      <td>None</td>\n",
       "      <td>milk chocolate, almonds, salt and natural color</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>22932</td>\n",
       "      <td>wythe will distributing company</td>\n",
       "      <td>None</td>\n",
       "      <td>corn syrup, sugar coconut oil, egg whites, sal...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>5968</td>\n",
       "      <td>p.c. woo, inc.</td>\n",
       "      <td>description_candy_candy</td>\n",
       "      <td>corn syrup, sugar, palm oil, citric acid, mono...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                            brand  \\\n",
       "241      266      russell stover candies inc.   \n",
       "29772  33073                     just goodies   \n",
       "516      580                 sulpice chocolat   \n",
       "20659  22932  wythe will distributing company   \n",
       "5350    5968                   p.c. woo, inc.   \n",
       "\n",
       "                                description  \\\n",
       "241    description_chocolate_russell stover   \n",
       "29772               description_candy_candy   \n",
       "516                                    None   \n",
       "20659                                  None   \n",
       "5350                description_candy_candy   \n",
       "\n",
       "                                             ingredients  serving_size  \\\n",
       "241    consist of milk chocolate1, sugar, whole milk,...          40.0   \n",
       "29772  sugar, corn syrup, confectioners glaze, honey,...          30.0   \n",
       "516      milk chocolate, almonds, salt and natural color          30.0   \n",
       "20659  corn syrup, sugar coconut oil, egg whites, sal...          42.0   \n",
       "5350   corn syrup, sugar, palm oil, citric acid, mono...          43.0   \n",
       "\n",
       "      serving_size_unit household_serving_fulltext  Calcium, Ca  \\\n",
       "241                   g                     pieces        150.0   \n",
       "29772                 g                        grm          0.0   \n",
       "516                   g                        grm        200.0   \n",
       "20659                 g                     pieces          0.0   \n",
       "5350                  g                     pieces          NaN   \n",
       "\n",
       "       Carbohydrate, by difference  Cholesterol  ...  \\\n",
       "241                          45.00         12.0  ...   \n",
       "29772                        96.67          0.0  ...   \n",
       "516                          50.00         23.0  ...   \n",
       "20659                        90.48          0.0  ...   \n",
       "5350                         86.05          0.0  ...   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_almond  \\\n",
       "241                                              0   \n",
       "29772                                            0   \n",
       "516                                              0   \n",
       "20659                                            0   \n",
       "5350                                             0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_almonds  \\\n",
       "241                                               0   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_trail mix  \\\n",
       "241                                                 0   \n",
       "29772                                               0   \n",
       "516                                                 0   \n",
       "20659                                               0   \n",
       "5350                                                0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_peanuts  \\\n",
       "241                                               0   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_mixed nuts  \\\n",
       "241                                                  0   \n",
       "29772                                                0   \n",
       "516                                                  0   \n",
       "20659                                                0   \n",
       "5350                                                 0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_dry roasted peanuts  \\\n",
       "241                                                    0          \n",
       "29772                                                  0          \n",
       "516                                                    0          \n",
       "20659                                                  0          \n",
       "5350                                                   0          \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_cashews  \\\n",
       "241                                               0   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_kernel  \\\n",
       "241                                              0   \n",
       "29772                                            0   \n",
       "516                                              0   \n",
       "20659                                            0   \n",
       "5350                                             0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_shell  \\\n",
       "241                                             0   \n",
       "29772                                           0   \n",
       "516                                             0   \n",
       "20659                                           0   \n",
       "5350                                            0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_pecan  \n",
       "241                                             0  \n",
       "29772                                           0  \n",
       "516                                             0  \n",
       "20659                                           0  \n",
       "5350                                            0  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_description(df):\n",
    "    desc_words = {CAKES:['tastykake', 'cupcakes', 'cheezecake', 'bakery fresh goodness', 'apple pie', 'pie', 'pecan pie',\n",
    "                                   'pumpkin pie', 'cake', 'cakes', 'cupcake', 'coffeecake', 'brownie', 'brownies', 'slice', 'sliced', \n",
    "                                    'torte', 'pies', 'donut', 'donuts', 'bakery'],\n",
    "    CANDY:['candy', 'candies', 'gummy', 'gummi', 'gummies', 'twist', 'stick', 'marshmallow', 'marshmallows',\n",
    "                       'jelly', 'snacks', \"sweet's\",  \"brach's\", 'cherry', 'strawberry', 'orange', 'watermelon', \n",
    "                       'peppermint', 'grape'],\n",
    "    CHIPS_PRETZELS:['potato chips', 'tortilla chips', 'kettle cooked potato chips', 'sea salt', 'kettle chips', \n",
    "                               \"snyder's of hanover\", 'sour cream & onion', 'wavy potato chips', \"herr's\", 'chips', 'chip', \n",
    "                               'tortilla', 'crisps', 'crisp', 'potato', 'pretzel', 'pretzels', 'fries', 'corn'],\n",
    "    CHOCOLATE:['dark chocolate', 'lindt', 'ghirardelli chocolate', 'russell stover', 'godiva', 'truffles', \n",
    "                          'dark chocolate bar', 'cocoa', 'praline', 'toffee', 'belgian', 'dark', 'truffle', 'chocolates'],\n",
    "    COOKIES:['cookie', 'cookies', 'chocolate chip', 'chocolate chip cookies', 'sandwich cookies', \n",
    "                       'shortbread cookies', 'frosted sugar cookies', 'sugar cookies''crackers', 'frosted', 'wafer', \n",
    "                       'cracker', 'biscuit', 'macaroon', 'waffle'],\n",
    "    POPCORN_PEANUTS:['popcorn', 'almond', 'almonds', 'trail mix', 'peanuts', 'mixed nuts', \n",
    "                                'dry roasted peanuts','cashews', 'kernel', 'shell', 'pecan']}\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    # Convert the dictionary values to sets to ensure uniqueness\n",
    "    data_dict = {key: set(values) for key, values in desc_words.items()}\n",
    "    # Create a new dictionary with the column names as keys and zeros as values\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in desc_words.items() for word in words}\n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    # Concatenate the existing DataFrame with the new DataFrame\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in desc_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['description'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['description'] = result_df.apply(lambda row: select_top_words(row, desc_words, 'description'), axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "extract_description(X_fe_w_cat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cde2d",
   "metadata": {},
   "source": [
    "# 'ingredients' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b16a8",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack ingredients, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06640f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for category in CATEGORIES:\n",
    "#    common_words_per_category_plot(category, 'ingredients', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed5d8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_common_words_per_column('ingredients', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7b04925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>serving_size_unit</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>Calcium, Ca</th>\n",
       "      <th>Carbohydrate, by difference</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>chocolate_butter</th>\n",
       "      <th>cookies_biscuits_baking soda</th>\n",
       "      <th>cookies_biscuits_enriched flour</th>\n",
       "      <th>cookies_biscuits_leavening</th>\n",
       "      <th>cookies_biscuits_eggs</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_almonds</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_peanuts</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_cashews</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_popcorn</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_pecans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>266</td>\n",
       "      <td>russell stover candies inc.</td>\n",
       "      <td>russell stover, nut cluster assortment</td>\n",
       "      <td>ingredients_chocolate_cocoa butter</td>\n",
       "      <td>40.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>150.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>33073</td>\n",
       "      <td>just goodies</td>\n",
       "      <td>candy corn</td>\n",
       "      <td>ingredients_candy_carnauba wax</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>580</td>\n",
       "      <td>sulpice chocolat</td>\n",
       "      <td>amande milk chocolate</td>\n",
       "      <td>ingredients_chocolate_milk chocolate</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>grm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>22932</td>\n",
       "      <td>wythe will distributing company</td>\n",
       "      <td>pumpkin face taffy</td>\n",
       "      <td>None</td>\n",
       "      <td>42.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>5968</td>\n",
       "      <td>p.c. woo, inc.</td>\n",
       "      <td>megatoys, happy easter basket, salt water taff...</td>\n",
       "      <td>ingredients_candy_red 40</td>\n",
       "      <td>43.0</td>\n",
       "      <td>g</td>\n",
       "      <td>pieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                            brand  \\\n",
       "241      266      russell stover candies inc.   \n",
       "29772  33073                     just goodies   \n",
       "516      580                 sulpice chocolat   \n",
       "20659  22932  wythe will distributing company   \n",
       "5350    5968                   p.c. woo, inc.   \n",
       "\n",
       "                                             description  \\\n",
       "241               russell stover, nut cluster assortment   \n",
       "29772                                         candy corn   \n",
       "516                                amande milk chocolate   \n",
       "20659                                 pumpkin face taffy   \n",
       "5350   megatoys, happy easter basket, salt water taff...   \n",
       "\n",
       "                                ingredients  serving_size serving_size_unit  \\\n",
       "241      ingredients_chocolate_cocoa butter          40.0                 g   \n",
       "29772        ingredients_candy_carnauba wax          30.0                 g   \n",
       "516    ingredients_chocolate_milk chocolate          30.0                 g   \n",
       "20659                                  None          42.0                 g   \n",
       "5350               ingredients_candy_red 40          43.0                 g   \n",
       "\n",
       "      household_serving_fulltext  Calcium, Ca  Carbohydrate, by difference  \\\n",
       "241                       pieces        150.0                        45.00   \n",
       "29772                        grm          0.0                        96.67   \n",
       "516                          grm        200.0                        50.00   \n",
       "20659                     pieces          0.0                        90.48   \n",
       "5350                      pieces          NaN                        86.05   \n",
       "\n",
       "       Cholesterol  ...  chocolate_butter  cookies_biscuits_baking soda  \\\n",
       "241           12.0  ...                 1                             0   \n",
       "29772          0.0  ...                 0                             0   \n",
       "516           23.0  ...                 0                             0   \n",
       "20659          0.0  ...                 0                             0   \n",
       "5350           0.0  ...                 0                             0   \n",
       "\n",
       "       cookies_biscuits_enriched flour  cookies_biscuits_leavening  \\\n",
       "241                                  0                           0   \n",
       "29772                                0                           0   \n",
       "516                                  0                           0   \n",
       "20659                                0                           0   \n",
       "5350                                 0                           0   \n",
       "\n",
       "       cookies_biscuits_eggs  popcorn_peanuts_seeds_related_snacks_almonds  \\\n",
       "241                        0                                             1   \n",
       "29772                      0                                             0   \n",
       "516                        0                                             1   \n",
       "20659                      0                                             0   \n",
       "5350                       0                                             0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_peanuts  \\\n",
       "241                                               1   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_cashews  \\\n",
       "241                                               1   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_popcorn  \\\n",
       "241                                               0   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_pecans  \n",
       "241                                              1  \n",
       "29772                                            0  \n",
       "516                                              0  \n",
       "20659                                            0  \n",
       "5350                                             0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_ingredients(df):\n",
    "    ingre_words = {CAKES:['leavening', 'eggs'],\n",
    "                   CANDY:['gelatin', 'carnauba wax', 'red 40', 'blue 1'],\n",
    "                   CHIPS_PRETZELS:['potatoes', 'onion powder', 'garlic powder', 'maltodextrin', 'yeast extract'],\n",
    "                   CHOCOLATE:['cocoa butter', 'milk chocolate', 'chocolate', 'milk', 'dark chocolate', 'butter'],\n",
    "                   COOKIES:['baking soda', 'enriched flour', 'leavening', 'eggs'],\n",
    "                   POPCORN_PEANUTS:['almonds', 'peanuts', 'cashews', 'popcorn', 'pecans']}\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    # Convert the dictionary values to sets to ensure uniqueness\n",
    "    data_dict = {key: set(values) for key, values in ingre_words.items()}\n",
    "    # Create a new dictionary with the column names as keys and zeros as values\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in ingre_words.items() for word in words}\n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    # Concatenate the existing DataFrame with the new DataFrame\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in ingre_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['ingredients'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['ingredients'] = result_df.apply(lambda row: select_top_words(row, ingre_words, 'ingredients'), axis=1)\n",
    "            \n",
    "    return result_df\n",
    "\n",
    "extract_ingredients(X_fe_w_cat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3a2be",
   "metadata": {},
   "source": [
    "# 'household_serving_fulltext' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cb942",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack household' column, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd386a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for category in CATEGORIES:\n",
    "#    common_words_per_category_plot(category, 'household_serving_fulltext', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa494764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most_common_words_per_column('household_serving_fulltext', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6189ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>serving_size_unit</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>Calcium, Ca</th>\n",
       "      <th>Carbohydrate, by difference</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>cookies_biscuits_cracker</th>\n",
       "      <th>cookies_biscuits_biscuit</th>\n",
       "      <th>cookies_biscuits_macaroon</th>\n",
       "      <th>cookies_biscuits_waffle</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_tbsp</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_kernel</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_popcorn</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_almond</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_shell</th>\n",
       "      <th>popcorn_peanuts_seeds_related_snacks_pecan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>266</td>\n",
       "      <td>russell stover candies inc.</td>\n",
       "      <td>russell stover, nut cluster assortment</td>\n",
       "      <td>consist of milk chocolate1, sugar, whole milk,...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>g</td>\n",
       "      <td>household_serving_fulltext_cakes_cupcakes_snac...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>33073</td>\n",
       "      <td>just goodies</td>\n",
       "      <td>candy corn</td>\n",
       "      <td>sugar, corn syrup, confectioners glaze, honey,...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>580</td>\n",
       "      <td>sulpice chocolat</td>\n",
       "      <td>amande milk chocolate</td>\n",
       "      <td>milk chocolate, almonds, salt and natural color</td>\n",
       "      <td>30.0</td>\n",
       "      <td>g</td>\n",
       "      <td>None</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>22932</td>\n",
       "      <td>wythe will distributing company</td>\n",
       "      <td>pumpkin face taffy</td>\n",
       "      <td>corn syrup, sugar coconut oil, egg whites, sal...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>g</td>\n",
       "      <td>household_serving_fulltext_cakes_cupcakes_snac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>5968</td>\n",
       "      <td>p.c. woo, inc.</td>\n",
       "      <td>megatoys, happy easter basket, salt water taff...</td>\n",
       "      <td>corn syrup, sugar, palm oil, citric acid, mono...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>g</td>\n",
       "      <td>household_serving_fulltext_cakes_cupcakes_snac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                            brand  \\\n",
       "241      266      russell stover candies inc.   \n",
       "29772  33073                     just goodies   \n",
       "516      580                 sulpice chocolat   \n",
       "20659  22932  wythe will distributing company   \n",
       "5350    5968                   p.c. woo, inc.   \n",
       "\n",
       "                                             description  \\\n",
       "241               russell stover, nut cluster assortment   \n",
       "29772                                         candy corn   \n",
       "516                                amande milk chocolate   \n",
       "20659                                 pumpkin face taffy   \n",
       "5350   megatoys, happy easter basket, salt water taff...   \n",
       "\n",
       "                                             ingredients  serving_size  \\\n",
       "241    consist of milk chocolate1, sugar, whole milk,...          40.0   \n",
       "29772  sugar, corn syrup, confectioners glaze, honey,...          30.0   \n",
       "516      milk chocolate, almonds, salt and natural color          30.0   \n",
       "20659  corn syrup, sugar coconut oil, egg whites, sal...          42.0   \n",
       "5350   corn syrup, sugar, palm oil, citric acid, mono...          43.0   \n",
       "\n",
       "      serving_size_unit                         household_serving_fulltext  \\\n",
       "241                   g  household_serving_fulltext_cakes_cupcakes_snac...   \n",
       "29772                 g                                               None   \n",
       "516                   g                                               None   \n",
       "20659                 g  household_serving_fulltext_cakes_cupcakes_snac...   \n",
       "5350                  g  household_serving_fulltext_cakes_cupcakes_snac...   \n",
       "\n",
       "       Calcium, Ca  Carbohydrate, by difference  Cholesterol  ...  \\\n",
       "241          150.0                        45.00         12.0  ...   \n",
       "29772          0.0                        96.67          0.0  ...   \n",
       "516          200.0                        50.00         23.0  ...   \n",
       "20659          0.0                        90.48          0.0  ...   \n",
       "5350           NaN                        86.05          0.0  ...   \n",
       "\n",
       "       cookies_biscuits_cracker  cookies_biscuits_biscuit  \\\n",
       "241                           0                         0   \n",
       "29772                         0                         0   \n",
       "516                           0                         0   \n",
       "20659                         0                         0   \n",
       "5350                          0                         0   \n",
       "\n",
       "       cookies_biscuits_macaroon  cookies_biscuits_waffle  \\\n",
       "241                            0                        0   \n",
       "29772                          0                        0   \n",
       "516                            0                        0   \n",
       "20659                          0                        0   \n",
       "5350                           0                        0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_tbsp  \\\n",
       "241                                            0   \n",
       "29772                                          0   \n",
       "516                                            0   \n",
       "20659                                          0   \n",
       "5350                                           0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_kernel  \\\n",
       "241                                              0   \n",
       "29772                                            0   \n",
       "516                                              0   \n",
       "20659                                            0   \n",
       "5350                                             0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_popcorn  \\\n",
       "241                                               0   \n",
       "29772                                             0   \n",
       "516                                               0   \n",
       "20659                                             0   \n",
       "5350                                              0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_almond  \\\n",
       "241                                              0   \n",
       "29772                                            0   \n",
       "516                                              0   \n",
       "20659                                            0   \n",
       "5350                                             0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_shell  \\\n",
       "241                                             0   \n",
       "29772                                           0   \n",
       "516                                             0   \n",
       "20659                                           0   \n",
       "5350                                            0   \n",
       "\n",
       "       popcorn_peanuts_seeds_related_snacks_pecan  \n",
       "241                                             0  \n",
       "29772                                           0  \n",
       "516                                             0  \n",
       "20659                                           0  \n",
       "5350                                            0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_household(df):\n",
    "    household_words = {CAKES:['cake', 'cakes', 'cupcakes', 'cupcake','brownie', 'pie', 'donut', 'muffin', 'tart', \n",
    "                              'torte', 'doughnut','slice', 'pastry', 'bun', 'loaf'],\n",
    "                       CANDY:['candies', 'candy','gummies', 'gummy', 'marshmallow', 'pop', 'twist', 'stick','bear'],\n",
    "                       CHIPS_PRETZELS:['chip', 'chips', 'fries', 'crisp', 'chipsabout', 'pretzelsabout'],\n",
    "                       CHOCOLATE:['squares', 'square', 'balls', 'ball', 'truffle', 'praline', 'pralines', 'block', 'tablet', 'bar'],\n",
    "                       COOKIES:['cookies', 'cookie', 'wafers', 'wafer', 'crackers', 'cracker', 'biscuit', 'macaroon', 'waffle'],\n",
    "                       POPCORN_PEANUTS:['tbsp', 'kernel', 'popcorn', 'almond', 'shell', 'pecan']}\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    # Convert the dictionary values to sets to ensure uniqueness\n",
    "    data_dict = {key: set(values) for key, values in household_words.items()}\n",
    "    # Create a new dictionary with the column names as keys and zeros as values\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in household_words.items() for word in words}\n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    # Concatenate the existing DataFrame with the new DataFrame\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in household_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['household_serving_fulltext'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['household_serving_fulltext'] = result_df.apply(lambda row: select_top_words(row, household_words, 'household_serving_fulltext'), axis=1)\n",
    "            \n",
    "    return result_df\n",
    "\n",
    "extract_household(X_fe_w_cat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f5e7d",
   "metadata": {},
   "source": [
    "# 'serving_size' column Research & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4d83b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Average Size</th>\n",
       "      <th>Median Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candy</td>\n",
       "      <td>32.025880</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cookies_biscuits</td>\n",
       "      <td>33.071227</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cakes_cupcakes_snack_cakes</td>\n",
       "      <td>74.718705</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chips_pretzels_snacks</td>\n",
       "      <td>29.253617</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>38.556940</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>popcorn_peanuts_seeds_related_snacks</td>\n",
       "      <td>31.747849</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Category  Average Size  Median Size\n",
       "0                                 candy     32.025880         35.0\n",
       "1                      cookies_biscuits     33.071227         30.0\n",
       "2            cakes_cupcakes_snack_cakes     74.718705         71.0\n",
       "3                 chips_pretzels_snacks     29.253617         28.0\n",
       "4                             chocolate     38.556940         40.0\n",
       "5  popcorn_peanuts_seeds_related_snacks     31.747849         30.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_by_category(df, category):\n",
    "    return df[df['category'] == category]\n",
    "\n",
    "def find_mean_and_median(category):\n",
    "    df = data_by_category(X_fe_w_cat, category)\n",
    "    return df['serving_size'].mean(), df['serving_size'].median()\n",
    "\n",
    "mean_median_df = {'Category': [CANDY, COOKIES, CAKES, CHIPS_PRETZELS, CHOCOLATE, POPCORN_PEANUTS],\n",
    "              'Average Size': [find_mean_and_median(CANDY)[0],find_mean_and_median(COOKIES)[0], find_mean_and_median(CAKES)[0],\n",
    "                               find_mean_and_median(CHIPS_PRETZELS)[0], find_mean_and_median(CHOCOLATE)[0],find_mean_and_median(POPCORN_PEANUTS)[0]],\n",
    "              'Median Size': [find_mean_and_median(CANDY)[1],find_mean_and_median(COOKIES)[1], find_mean_and_median(CAKES)[1],\n",
    "                               find_mean_and_median(CHIPS_PRETZELS)[1], find_mean_and_median(CHOCOLATE)[1],find_mean_and_median(POPCORN_PEANUTS)[1]]}\n",
    "size_df = pd.DataFrame(mean_median_df)\n",
    "\n",
    "size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686d72e",
   "metadata": {},
   "source": [
    "It seems that product from cakes category are much heavy then all the other products. Therefore, this column may help us distinguish between cakes category and all the others, so we'll keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc263bb7",
   "metadata": {},
   "source": [
    "# 'serving_size_unit' column Research & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c29f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g</td>\n",
       "      <td>31743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8554</th>\n",
       "      <td>ml</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values  Frequency\n",
       "0         g      31743\n",
       "8554     ml          8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_size_unit = {'values': data['serving_size_unit'].drop_duplicates(),\n",
    "                      'Frequency': [data['serving_size_unit'].value_counts()['g'], \n",
    "                                    data['serving_size_unit'].value_counts()['ml']]\n",
    "                    }\n",
    "serving_size_unit_df = pd.DataFrame(serving_size_unit)\n",
    "serving_size_unit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b03b75",
   "metadata": {},
   "source": [
    "We notice that most of the snacks has the value 'g' and just 8 of them has the value 'ml'.\n",
    "Therefore, \"size_unit\" column is not important and has no effect on the data, so it can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "606f35c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>brand</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>serving_size</th>\n",
       "      <th>household_serving_fulltext</th>\n",
       "      <th>Calcium, Ca</th>\n",
       "      <th>Carbohydrate, by difference</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Energy</th>\n",
       "      <th>...</th>\n",
       "      <th>Fatty acids, total trans</th>\n",
       "      <th>Fiber, total dietary</th>\n",
       "      <th>Iron, Fe</th>\n",
       "      <th>Protein</th>\n",
       "      <th>Sodium, Na</th>\n",
       "      <th>Sugars, total including NLEA</th>\n",
       "      <th>Total lipid (fat)</th>\n",
       "      <th>Vitamin A, IU</th>\n",
       "      <th>Vitamin C, total ascorbic acid</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>266</td>\n",
       "      <td>russell stover candies inc.</td>\n",
       "      <td>russell stover, nut cluster assortment</td>\n",
       "      <td>consist of milk chocolate1, sugar, whole milk,...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>pieces</td>\n",
       "      <td>150.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>12.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>33073</td>\n",
       "      <td>just goodies</td>\n",
       "      <td>candy corn</td>\n",
       "      <td>sugar, corn syrup, confectioners glaze, honey,...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>grm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>183.0</td>\n",
       "      <td>76.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>candy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>580</td>\n",
       "      <td>sulpice chocolat</td>\n",
       "      <td>amande milk chocolate</td>\n",
       "      <td>milk chocolate, almonds, salt and natural color</td>\n",
       "      <td>30.0</td>\n",
       "      <td>grm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>433.0</td>\n",
       "      <td>46.67</td>\n",
       "      <td>40.00</td>\n",
       "      <td>333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20659</th>\n",
       "      <td>22932</td>\n",
       "      <td>wythe will distributing company</td>\n",
       "      <td>pumpkin face taffy</td>\n",
       "      <td>corn syrup, sugar coconut oil, egg whites, sal...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>pieces</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>131.0</td>\n",
       "      <td>54.76</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>candy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>5968</td>\n",
       "      <td>p.c. woo, inc.</td>\n",
       "      <td>megatoys, happy easter basket, salt water taff...</td>\n",
       "      <td>corn syrup, sugar, palm oil, citric acid, mono...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>pieces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>81.0</td>\n",
       "      <td>46.51</td>\n",
       "      <td>5.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>candy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                            brand  \\\n",
       "241      266      russell stover candies inc.   \n",
       "29772  33073                     just goodies   \n",
       "516      580                 sulpice chocolat   \n",
       "20659  22932  wythe will distributing company   \n",
       "5350    5968                   p.c. woo, inc.   \n",
       "\n",
       "                                             description  \\\n",
       "241               russell stover, nut cluster assortment   \n",
       "29772                                         candy corn   \n",
       "516                                amande milk chocolate   \n",
       "20659                                 pumpkin face taffy   \n",
       "5350   megatoys, happy easter basket, salt water taff...   \n",
       "\n",
       "                                             ingredients  serving_size  \\\n",
       "241    consist of milk chocolate1, sugar, whole milk,...          40.0   \n",
       "29772  sugar, corn syrup, confectioners glaze, honey,...          30.0   \n",
       "516      milk chocolate, almonds, salt and natural color          30.0   \n",
       "20659  corn syrup, sugar coconut oil, egg whites, sal...          42.0   \n",
       "5350   corn syrup, sugar, palm oil, citric acid, mono...          43.0   \n",
       "\n",
       "      household_serving_fulltext  Calcium, Ca  Carbohydrate, by difference  \\\n",
       "241                       pieces        150.0                        45.00   \n",
       "29772                        grm          0.0                        96.67   \n",
       "516                          grm        200.0                        50.00   \n",
       "20659                     pieces          0.0                        90.48   \n",
       "5350                      pieces          NaN                        86.05   \n",
       "\n",
       "       Cholesterol  Energy  ...  Fatty acids, total trans  \\\n",
       "241           12.0   575.0  ...                       0.0   \n",
       "29772          0.0   367.0  ...                       0.0   \n",
       "516           23.0   560.0  ...                       0.0   \n",
       "20659          0.0   381.0  ...                       0.0   \n",
       "5350           0.0   395.0  ...                       0.0   \n",
       "\n",
       "       Fiber, total dietary  Iron, Fe  Protein  Sodium, Na  \\\n",
       "241                     5.0       2.7    12.50        50.0   \n",
       "29772                   0.0       0.0     6.67       183.0   \n",
       "516                     3.3       1.2    10.00       433.0   \n",
       "20659                   0.0       0.0     0.00       131.0   \n",
       "5350                    0.0       NaN     0.00        81.0   \n",
       "\n",
       "       Sugars, total including NLEA  Total lipid (fat)  Vitamin A, IU  \\\n",
       "241                           35.00              40.00          250.0   \n",
       "29772                         76.67               0.00            0.0   \n",
       "516                           46.67              40.00          333.0   \n",
       "20659                         54.76               4.76            0.0   \n",
       "5350                          46.51               5.81            NaN   \n",
       "\n",
       "       Vitamin C, total ascorbic acid   category  \n",
       "241                               0.0  chocolate  \n",
       "29772                             0.0      candy  \n",
       "516                               0.0  chocolate  \n",
       "20659                             0.0      candy  \n",
       "5350                              NaN      candy  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_size_unit_column(df):\n",
    "    return df.drop(['serving_size_unit'],axis=1)\n",
    "\n",
    "drop_size_unit_column(X_fe_w_cat).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac807e1",
   "metadata": {},
   "source": [
    "# Nutrients columns Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e938611",
   "metadata": {},
   "source": [
    "# Part 2 - Image handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba13ace",
   "metadata": {},
   "source": [
    "We will build a CNN, then calculating the probabilities vector and adding it to the tabular data [Edit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bc33c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(label_path)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load the numpy arrays of training and testing image paths\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mX_y_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     18\u001b[0m test_image_paths \u001b[38;5;241m=\u001b[39m X_y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Extract category labels from paths\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'path'"
     ]
    }
   ],
   "source": [
    "# Set the paths\n",
    "train_base_directory = 'images/train/'  # Replace with the actual train path\n",
    "test_base_directory = 'images/train/'  # Replace with the actual test path\n",
    "\n",
    "train_output_directory = 'images/train_categorized/'  # Replace with your desired train output path\n",
    "test_output_directory = 'images/test_categorized/'  # Replace with your desired test output path\n",
    "\n",
    "# Function to create folders for each category\n",
    "def create_category_folders(output_directory, unique_labels):\n",
    "    for label in unique_labels:\n",
    "        label_path = os.path.join(output_directory, label)\n",
    "        if os.path.exists(label_path):\n",
    "            shutil.rmtree(label_path)  # Delete existing folder\n",
    "        os.makedirs(label_path)\n",
    "\n",
    "# Load the numpy arrays of training and testing image paths\n",
    "train_image_paths = X_y_train['path'].unique()\n",
    "test_image_paths = X_y_test['path'].unique()\n",
    "\n",
    "# Extract category labels from paths\n",
    "def extract_category_label(image_path):\n",
    "    category = os.path.basename(os.path.dirname(image_path))\n",
    "    return category\n",
    "\n",
    "# Create folders for each category in the train output directory\n",
    "unique_train_labels = np.unique([extract_category_label(path) for path in train_image_paths])\n",
    "create_category_folders(train_output_directory, unique_train_labels)\n",
    "\n",
    "# Copy train images to categorized folders\n",
    "for image_path in train_image_paths:\n",
    "    category = extract_category_label(image_path)\n",
    "    output_folder = os.path.join(train_output_directory, category)\n",
    "    shutil.copy(image_path, output_folder)\n",
    "\n",
    "print(\"Train images copied to categorized folders.\")\n",
    "\n",
    "# Create folders for each category in the test output directory\n",
    "unique_test_labels = np.unique([extract_category_label(path) for path in test_image_paths])\n",
    "create_category_folders(test_output_directory, unique_test_labels)\n",
    "\n",
    "# Copy test images to categorized folders\n",
    "for image_path in test_image_paths:\n",
    "    category = extract_category_label(image_path)\n",
    "    output_folder = os.path.join(test_output_directory, category)\n",
    "    shutil.copy(image_path, output_folder)\n",
    "\n",
    "print(\"Test images copied to categorized folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths and image size\n",
    "train_data_dir = 'images/train'\n",
    "image_dir = 'images/train'\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split data into training and validation\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "\n",
    "# Load and preprocess training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset of data\n",
    ")\n",
    "\n",
    "\n",
    "# Load and preprocess validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset of data\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset of data for testing\n",
    ")\n",
    "\n",
    "eval_result = model.evaluate(test_generator)\n",
    "print(\"Test Loss:\", eval_result[0])\n",
    "print(\"Test Accuracy:\", eval_result[1])\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ad2fe",
   "metadata": {},
   "source": [
    "We wrote a regular .py script that will export the probabilitis vector to .csv file [Edit]  \n",
    "Lets read it and add it to the tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0fa53b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'photos_probs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m photos_probs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphotos_probs.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m photos_probs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(idx))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'photos_probs.csv'"
     ]
    }
   ],
   "source": [
    "photos_probs = pd.read_csv('photos_probs.csv')\n",
    "idx = photos_probs['idx'].unique()\n",
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b6a31",
   "metadata": {},
   "source": [
    "[We didnt get probs for all the 30K rows,\n",
    "So we have 2 df, one with 30K rows and without the images probs\n",
    "The second will have 20K rows but with images probs\n",
    "We will decide based on the following results on which we should foucus-Edit!!!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_probs = pd.merge(data, photos_probs, how=\"left\",on='idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28744e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.copy()\n",
    "df2 = data_with_probs[data_with_probs['idx'].isin(idx)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468d344",
   "metadata": {},
   "source": [
    "# images --- need to delete this at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81758ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path(row):\n",
    "    idx = str(row['idx'])\n",
    "    category = row['category']\n",
    "    path = f\"images/train/{category}/{idx}.jpg\"\n",
    "    return path\n",
    "\n",
    "X_y_train['path'] = X_y_train.apply(path,axis=1)\n",
    "X_y_test['path'] = X_y_test.apply(path,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631169c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Set the paths\n",
    "train_base_directory = 'images/train/'  # Replace with the actual train path\n",
    "test_base_directory = 'images/train/'  # Replace with the actual test path\n",
    "\n",
    "train_output_directory = 'images/train_categorized/'  # Replace with your desired train output path\n",
    "test_output_directory = 'images/test_categorized/'  # Replace with your desired test output path\n",
    "\n",
    "# Function to create folders for each category\n",
    "def create_category_folders(output_directory, unique_labels):\n",
    "    for label in unique_labels:\n",
    "        label_path = os.path.join(output_directory, label)\n",
    "        if os.path.exists(label_path):\n",
    "            shutil.rmtree(label_path)  # Delete existing folder\n",
    "        os.makedirs(label_path)\n",
    "\n",
    "# Load the numpy arrays of training and testing image paths\n",
    "train_image_paths = X_y_train['path'].unique()\n",
    "test_image_paths = X_y_test['path'].unique()\n",
    "\n",
    "# Extract category labels from paths\n",
    "def extract_category_label(image_path):\n",
    "    category = os.path.basename(os.path.dirname(image_path))\n",
    "    return category\n",
    "\n",
    "# Create folders for each category in the train output directory\n",
    "unique_train_labels = np.unique([extract_category_label(path) for path in train_image_paths])\n",
    "create_category_folders(train_output_directory, unique_train_labels)\n",
    "\n",
    "# Copy train images to categorized folders\n",
    "for image_path in train_image_paths:\n",
    "    category = extract_category_label(image_path)\n",
    "    output_folder = os.path.join(train_output_directory, category)\n",
    "    shutil.copy(image_path, output_folder)\n",
    "\n",
    "print(\"Train images copied to categorized folders.\")\n",
    "\n",
    "# Create folders for each category in the test output directory\n",
    "unique_test_labels = np.unique([extract_category_label(path) for path in test_image_paths])\n",
    "create_category_folders(test_output_directory, unique_test_labels)\n",
    "\n",
    "# Copy test images to categorized folders\n",
    "for image_path in test_image_paths:\n",
    "    category = extract_category_label(image_path)\n",
    "    output_folder = os.path.join(test_output_directory, category)\n",
    "    shutil.copy(image_path, output_folder)\n",
    "\n",
    "print(\"Test images copied to categorized folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths and image size\n",
    "train_data_dir = 'images/train'\n",
    "image_dir = 'images/train'\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Data augmentation for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split data into training and validation\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "\n",
    "# Load and preprocess training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset of data\n",
    ")\n",
    "\n",
    "\n",
    "# Load and preprocess validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset of data\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "model.fit(train_generator, epochs=num_epochs, validation_data=validation_generator)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset of data for testing\n",
    ")\n",
    "\n",
    "eval_result = model.evaluate(test_generator)\n",
    "print(\"Test Loss:\", eval_result[0])\n",
    "print(\"Test Accuracy:\", eval_result[1])\n",
    "\n",
    "\n",
    "# Save the model\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf17b7",
   "metadata": {},
   "source": [
    "# Part 3 - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77438670",
   "metadata": {},
   "source": [
    "# FE CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67c75a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = FunctionTransformer(extract_brand)#,kw_args={'a':5})\n",
    "# Description1 = FunctionTransformer(desc_indication_columns,kw_args={'b':5})\n",
    "Description2 = FunctionTransformer(extract_description)\n",
    "# Ingredients1 = FunctionTransformer(ingre_indication_columns,kw_args={'c':5})\n",
    "Ingredients2 = FunctionTransformer(extract_ingredients)\n",
    "# Household1 = FunctionTransformer(household_indication_columns,kw_args={'d':5})\n",
    "Household2 = FunctionTransformer(extract_household)\n",
    "Drop = FunctionTransformer(drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [\"brand\", \"description\", \"ingredients\",\"household_serving_fulltext\"]\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, X.select_dtypes(include=['int64', 'float64']).columns),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('brand',Brand),\n",
    "    ('description2',Description2),\n",
    "    ('ingredients2',Ingredients2),\n",
    "    ('household2',Household2),\n",
    "    ('drop',Drop),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea071e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fe_train['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "# Run 5-fold cross-validation\n",
    "scores = cross_val_score(pipeline, X_fe_train, y_fe_train.values.ravel(), cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac88e3",
   "metadata": {},
   "source": [
    "### Let's fit on all X_fe_train and check the test acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "03a8579b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_fe_train, y_fe_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_fe_train, y_fe_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fe_test['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "\n",
    "test_score = pipeline.score(X_fe_test, y_fe_test.values.ravel())\n",
    "print(\"Test score:\", test_score)\n",
    "\n",
    "predicted_labels = pipeline.predict(X_fe_test)\n",
    "\n",
    "accuracy = accuracy_score(y_fe_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_fe_test, predicted_labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c89237",
   "metadata": {},
   "source": [
    "### Maybe add this to gridsearch(choose the best words from the dicts)[Edit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'brand__kw_args':[{'a':5},{'a':10},{'a':15},{'a':20}]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    "#  'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "#                         [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "# results = {}\n",
    "# for name, clf in classifiers:\n",
    "#     pipeline.set_params(classifier=clf)\n",
    "    \n",
    "#     grid_search = GridSearchCV(\n",
    "#         pipeline,\n",
    "#         param_grid=param_grid,\n",
    "#         cv=5,\n",
    "#         n_jobs=-1,\n",
    "#         verbose = 3,\n",
    "#         scoring = 'accuracy'\n",
    "#     )\n",
    "    \n",
    "#     grid_search.fit(X_train, y_train.values.ravel())\n",
    "#     results[name] = grid_search\n",
    "\n",
    "# for name, grid_search in results.items():\n",
    "#     print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Best score for {name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    "#  'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "#                         [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "# predicted_labels = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, predicted_labels)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# report = classification_report(y_test, predicted_labels)\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d676f",
   "metadata": {},
   "source": [
    "### So we got a different parameter for brand for each model.\n",
    "### We will take that into consideration in the model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1a982",
   "metadata": {},
   "source": [
    "## SMOTE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data\n",
    "X = X_mt\n",
    "y = y_mt\n",
    "\n",
    "y['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "# Define model\n",
    "model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('brand',Brand),\n",
    "    ('description2',Description2),\n",
    "    ('ingredients2',Ingredients2),\n",
    "    ('household2',Household2),\n",
    "    ('drop',Drop),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# Train a model\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_resampled, y_resampled.values.ravel())\n",
    "\n",
    "\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0c289",
   "metadata": {},
   "source": [
    "## MT CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mt_train['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "# Define the classifiers\n",
    "classifiers = [\n",
    "    ('XGB', XGBClassifier()),\n",
    "    ('HistGradientBoosting', HistGradientBoostingClassifier()),\n",
    "    ('RandomForest', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('brand',Brand),\n",
    "    ('description2',Description2),\n",
    "    ('ingredients2',Ingredients2),\n",
    "    ('household2',Household2),\n",
    "    ('drop',Drop),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "# Define hyperparameters to search for each classifier\n",
    "param_grids = {\n",
    "    'XGB': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'classifier__max_iter': [50, 100, 200],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers:\n",
    "    pipeline.set_params(classifier=clf)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        n_jobs=-1,\n",
    "        verbose = 3,\n",
    "        scoring = 'accuracy'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_mt_train, y_mt_train.values.ravel())  # Replace with your data\n",
    "    results[name] = grid_search\n",
    "\n",
    "# Print the best parameters and best score for each classifier\n",
    "for name, grid_search in results.items():\n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {name}: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e326c75",
   "metadata": {},
   "source": [
    "# Train the models on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1823517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
