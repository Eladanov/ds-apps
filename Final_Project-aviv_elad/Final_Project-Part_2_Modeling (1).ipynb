{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0272e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 15:11:28.204666: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import combinations # not mandatory\n",
    "import imutils\n",
    "from imutils import build_montages, paths\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import cv, XGBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ded158",
   "metadata": {},
   "source": [
    "## Cleaning and Arranging the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8eecc",
   "metadata": {},
   "source": [
    "At first we want to remove unnecessary signs from the data, rearrange some of the columns that it will be easer to learn from it and train the model on it. Therefore, we define useful functions for cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e604557-4d3a-4652-801d-4acd3ab3e77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a002848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses(string):\n",
    "    pattern = r'\\([^()]*\\)'  # Matches \"(...)\" pattern\n",
    "    while re.search(pattern, string):\n",
    "        string = re.sub(pattern, '', string)\n",
    "    return string.strip()\n",
    "\n",
    "def format_ingredients(string):\n",
    "    string = string.replace('.', '')  # Remove periods\n",
    "    string = re.sub(r'\\s*,\\s*', ',', string)  # Remove spaces after commas\n",
    "    ingredients_list = string.split(',')  # Split by comma\n",
    "    formatted_ingredients = [ingredient.strip() for ingredient in ingredients_list]  # Remove leading/trailing spaces for each ingredient\n",
    "    return ', '.join(formatted_ingredients)  # Join formatted ingredients with commas\n",
    "\n",
    "def clean_text_values(df):\n",
    "    text_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "    for column in text_columns:\n",
    "        if column == 'category':\n",
    "            continue\n",
    "        df[column].fillna('NA', inplace=True)\n",
    "        df[column] = df[column].map(str.lower)\n",
    "        if column == 'ingredients':\n",
    "            df[column] = df[column].apply(remove_parentheses)\n",
    "            df[column] = df[column].apply(format_ingredients)\n",
    "        if column == 'household_serving_fulltext':\n",
    "            df[column] = df[column].map(lambda x: re.sub('[^a-z]+', '', x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7df747",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208e40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_train = pd.read_csv('data/food_train.csv')\n",
    "food_test = pd.read_csv('data/food_test.csv')\n",
    "food_nutrients = pd.read_csv('data/food_nutrients.csv')\n",
    "nutrients_names = pd.read_csv('data/nutrients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f7d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_values(food_train)\n",
    "clean_text_values(food_test)\n",
    "nutrients = pd.merge(food_nutrients, nutrients_names, how='left',on='nutrient_id')\n",
    "pivoted_nutrients = pd.pivot_table(nutrients, values='amount', index='idx', columns='name')\n",
    "data = pd.merge(food_train, pivoted_nutrients, how='left', on='idx')\n",
    "data_test = pd.merge(food_test, pivoted_nutrients, how='left', on='idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea9ac8",
   "metadata": {},
   "source": [
    "### Remove columns with > 0.8 nulls\n",
    "\n",
    "We don't want that missing data will hurt our data analysis. Hence, we decided that if there is a product which 80% of the his data is missing, we will ignore it and don't use it to get our conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fe0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = data.columns[data.isnull().mean() > 0.8]\n",
    "data = data.drop(columns=cols_to_remove)\n",
    "\n",
    "cols_to_remove_test = data_test.columns[data_test.isnull().mean() > 0.8]\n",
    "data_test = data_test.drop(columns=cols_to_remove_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764ef73e",
   "metadata": {},
   "source": [
    "### Enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4fa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDY = 'candy'\n",
    "COOKIES = 'cookies_biscuits'\n",
    "CAKES = 'cakes_cupcakes_snack_cakes'\n",
    "CHIPS_PRETZELS = 'chips_pretzels_snacks'\n",
    "CHOCOLATE = 'chocolate'\n",
    "POPCORN_PEANUTS = 'popcorn_peanuts_seeds_related_snacks'\n",
    "\n",
    "CATEGORIES = [CANDY, COOKIES, CAKES, CHIPS_PRETZELS, CHOCOLATE, POPCORN_PEANUTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56613f57-eb92-47f3-98a6-88e65bf6047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3525"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['idx'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1789872",
   "metadata": {},
   "source": [
    "# Our way of work:\n",
    "\n",
    "At first we will work on each type of data separately.  \n",
    "*Part 1 - Images data:* We look over the train and test images for all snack products and build Convolutional Neural Network (CNN).  \n",
    "*Part 2 - Tabular data:* We look over the 3 data sets - food_train.csv and food_test.csv, nutrients.csv and food_nutrients.csv. In this part we will handle each column and analyze the data.  \n",
    "*Part 3* - Modeling: We will train the data and try to get the best model for Predicting the test products' category.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e93085",
   "metadata": {},
   "source": [
    "# Part 1 - Images data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f721d",
   "metadata": {},
   "source": [
    "We built a CNN in different regular python file, using tensorflow model (as we saw in class) with the images data, as you can in \"CNN.py\".  \n",
    "Then we calculate the probability for each image to be belong to each category and save it in a probabilities vector. Then, we export the probabilitis vector and save it in .csv file called 'photos_probs.csv'.  \n",
    "Finally, we chose to add it to the tabular data .\n",
    "\n",
    "So, now lets read it and add it to the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5255e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22077\n"
     ]
    }
   ],
   "source": [
    "photos_probs = pd.read_csv('photos_probs.csv')\n",
    "photo_test_probs = pd.read_csv('photos_test_probs.csv')\n",
    "idx = photos_probs['idx'].unique()\n",
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bffc7d",
   "metadata": {},
   "source": [
    "Unfortunately, we didn't get probs for all the 30K rows.   \n",
    "Therefore, we created 2 df for the data analysis part: \n",
    "- One with 30K rows and without the images probs (df1).   \n",
    "- second with 20K rows but with the images probs (df2).  \n",
    "\n",
    "We will decide based on the following results on which we should foucus.  \n",
    "In Part 3 - We will try to use the images probs in a model and check if it helps for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f24328dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_probs = pd.merge(data, photos_probs, how=\"left\",on='idx')\n",
    "data_with_probs_test = pd.merge(data_test, photo_test_probs, how=\"left\",on='idx')\n",
    "df1 = data.copy()\n",
    "df2 = data_with_probs[data_with_probs['idx'].isin(idx)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7399051",
   "metadata": {},
   "source": [
    "As we told in the instruction, the classes are not balanced but they are not very imbalanced either. We will use later in SMOTE method to deal with that.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907be36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popcorn_peanuts_seeds_related_snacks    7645\n",
       "candy                                   7584\n",
       "cookies_biscuits                        5284\n",
       "cakes_cupcakes_snack_cakes              3786\n",
       "chocolate                               3772\n",
       "chips_pretzels_snacks                   3680\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae706ab0",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "We'll split df1 to train and test sets. Then, we'll split the df1_train set into 2 data sets. One for feature engineering and the other for model tuning.  \n",
    "We will use the FE data set also for fillNA methods and the modeltuning set for resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b407f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.loc[:,df1.columns != 'category']\n",
    "y = df1.loc[:,['category']]\n",
    "\n",
    "y['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_fe, X_mt, y_fe, y_mt = train_test_split(X_train, y_train, test_size=0.4, random_state=42, stratify=y_train)\n",
    "\n",
    "X_fe_train, X_fe_test, y_fe_train, y_fe_test = train_test_split(X_fe, y_fe, test_size=0.2, random_state=42, stratify=y_fe)\n",
    "X_mt_train, X_mt_test, y_mt_train, y_mt_test = train_test_split(X_mt, y_mt, test_size=0.2, random_state=42, stratify=y_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9a465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_y_train = pd.merge(X_train, y_train, how=\"left\",left_index=True, right_index=True)\n",
    "# X_y_train['idx'] = X_y_train['idx'].map(lambda x: str(x))\n",
    "# X_y_test = pd.merge(X_test, y_test, how=\"left\",left_index=True, right_index=True)\n",
    "# X_y_test['idx'] = X_y_test['idx'].map(lambda x: str(x))\n",
    "\n",
    "# X_y_mt = pd.merge(X_mt, y_mt, how=\"left\",left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "#### #### concat for eda\n",
    "X_fe_w_cat = pd.merge(X_fe_train, y_fe_train, how=\"left\",left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51b264",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f745de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_name(df, col_name):\n",
    "    df[col_name] = df[col_name].map(lambda x: str(x).translate(str.maketrans('', '', string.punctuation)))\n",
    "    return(df)\n",
    "\n",
    "def data_by_category(df, category):\n",
    "    return df[df['category'] == category]\n",
    "\n",
    "def select_top_words(row, dict_words, column):\n",
    "    desc = row[column]\n",
    "    for category, words in dict_words.items():\n",
    "        for word in dict_words[category]:\n",
    "            if word in desc:\n",
    "                name = f\"{column}_{category}_{word}\"\n",
    "                return name\n",
    "    return 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8b2e",
   "metadata": {},
   "source": [
    "# Part 1 - Feature Engineering\n",
    "\n",
    "### Top 15 words in each column in the data per category\n",
    "We have found the most 15 common words in each column: 'brand', 'description', 'ingredients' and 'household' per category. That will help us understand which words are represent and imply each category.\n",
    "\n",
    "You can see all the data analysis we performed in the \"Final_Project-Part_2-Words_Selection\" notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c02e3",
   "metadata": {},
   "source": [
    "# 'brand' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9af844",
   "metadata": {},
   "source": [
    "We will look on top 15 common brands over each category. After that we will change the brand column and make sure that just the selected top 15 (from each category) brands will stay the same and all the other will sign as unbranded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2aca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_brand(df):\n",
    "    brand_words =     brand_words = {\n",
    "    \"candy\": [\"ferrara candy company\",\"frankford candy llc\",\"sunmark\",\"mars chocolate north america llc\",\"just born inc\",\"ross acquisition inc\",\n",
    "              \"russell stover candies inc\",\"maud borup inc\",\"reeses\",\"tops markets llc\",\"supervalu inc\",\"weis markets inc\",\"tootsie roll industries inc\",\n",
    "              \"holiday candy corp inc\",\"wm wrigley jr company\",\n",
    "    ],\n",
    "    \"cookies\": [\"nabisco biscuit company\",\"keebler company\",\"lofthouse foods\",\"ahold usa inc\",\"bimbo bakeries usa inc\",\"safeway inc\",\"wegmans food markets inc\",\n",
    "                \"lenny  larrys\",\"abimar foods inc\",\"kingston marketing co\",\n",
    "    ],\n",
    "    \"cakes\": [\"mckee foods corporation\",\"hostess brands llc\",\"tasty baking company\",\"bimbo bakeries usa inc\",\"sweet ps bake shop\",\"twobite\",\"schnuck markets inc\",\n",
    "              \"fresh  easy\",\"dawn food products inc\",\"dierbergs markets inc\",\"flowers foods inc\",\"labrees bakery inc\",\"rich products corporation\",\n",
    "              \"aryzta llc\",\"rocky mountain pies\",\n",
    "    ],\n",
    "    \"chips_pretzels\": [\"utz quality foods inc\",\"the hain celestial group inc\",\"herr foods inc\",\"snyderslance inc\",\"jays foods inc\",\"cape cod potato chips inc\",\n",
    "                       \"inventure foods inc\",\"old dutch foods inc\",\"better made snack foods inc\",\"small planet foods inc\",\"roundys\",\"pinnacle foods group llc\",\n",
    "    ],\n",
    "    \"chocolate\": [\"lindt  sprungli schweiz ag\",\"russell stover candies inc\",\"mars chocolate north america llc\",\"godiva chocolatier inc\",\"ghirardelli chocolate company\",\n",
    "                  \"frankford candy llc\",\"moonstruck chocolate co\",\"ross acquisition inc\",\"rm palmer co\",\"hammonds candies since 1920 llc\",\"theo chocolate inc\",\n",
    "                  \"demets candy company\",\"ghirardelli\",\"green  blacks\",\"fannie may confections inc\",\"nestle usa inc\",\"harmons inc\",\"whitmans candies inc\",\n",
    "    ],\n",
    "    \"popcorn_peanuts\": [\"american importing co inc\",\"nabisco food company\",\"john b sanfilippo  son inc\",\"tops markets llc\",\"star snacks co inc\",\n",
    "                        \"safeway inc\",\"supervalu inc\",\"ahold usa inc\",\"weis markets inc\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "    num_rows = len(df)\n",
    "    data_dict = {key: set(values) for key, values in brand_words.items()}\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in brand_words.items() for word in words}\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in brand_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['brand'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['brand'] = result_df.apply(lambda row: select_top_words(row, brand_words, 'brand'), axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da0519",
   "metadata": {},
   "source": [
    "# 'description' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a4b0",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack description, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3504a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(df):\n",
    "    desc_words = { CAKES:['cake', 'cakes', 'tastykake', 'cupcakes', 'cheezecake', 'bakery fresh goodness', 'apple pie', 'pie', \n",
    "           'pecan pie', 'pumpkin pie', 'pies', 'cupcake', 'coffeecake', 'brownie', 'brownies', 'slice', 'sliced', \n",
    "           'torte', 'donut', 'donuts', 'bakery', 'eclair'],\n",
    "    CANDY:['candy', 'candies', 'gummy', 'gummi', 'gummies', 'twist', 'stick', 'sticks', 'marshmallow', 'marshmallows',\n",
    "           'jelly', 'jell', 'snacks', \"sweet's\",  \"brach's\", 'cherry', 'strawberry', 'orange', 'watermelon', \n",
    "           'peppermint', 'grape', 'lolli', 'fruit', 'licorice', 'drops', 'confection', 'chicks', 'sour', 'sweet', \n",
    "           'peeps', 'dragee', 'fizz', 'patties', 'cane', 'chewy'],\n",
    "    CHIPS_PRETZELS:['potato chips', 'tortilla chips', 'kettle cooked potato chips', 'sea salt', 'kettle chips', 'kettle',\n",
    "                    \"snyder's of hanover\", 'sour cream & onion', 'wavy potato chips', \"herr's\", 'chips', 'chip', \n",
    "                    'tortilla', 'crisps', 'crisp', 'potato', 'pretzel', 'pretzels', 'fries'],\n",
    "    CHOCOLATE:[ 'chocolate', 'chocolates', 'dark chocolate', 'lindt', 'ghirardelli chocolate', 'russell stover', 'godiva',  \n",
    "                'truffle', 'truffles', 'dark chocolate bar', 'cocoa', 'praline', 'toffee', 'belgian', 'dark', 'caramel'],\n",
    "    COOKIES:['cookie', 'cookies', 'chocolate chip', 'chocolate chip cookies', 'sandwich cookies', 'sandwich', 'shortbread cookies',\n",
    "             'frosted sugar cookies', 'sugar cookies', 'cracker', 'crackers', 'frosted', 'wafer', 'wafers', 'biscuit', \n",
    "             'macaroon', 'waffle', 'gingernread', 'macarons'],\n",
    "    POPCORN_PEANUTS:['popcorn', 'almond', 'almonds', 'mix', 'trail mix', 'peanuts', 'mixed nuts', 'nuts', \"pistachios\",\n",
    "                    'dry roasted peanuts', 'roasted', 'cashews', 'kernel', 'shell', 'pecan', 'seeds', 'macadamias', \n",
    "                     'corn', 'nutty']}\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    data_dict = {key: set(values) for key, values in desc_words.items()}\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in desc_words.items() for word in words}\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in desc_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['description'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['description'] = result_df.apply(lambda row: select_top_words(row, desc_words, 'description'), axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cde2d",
   "metadata": {},
   "source": [
    "# 'ingredients' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b16a8",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack ingredients, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b04925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients(df):\n",
    "    ingre_words = {\n",
    "        CAKES:['leavening', 'eggs', 'water', 'soybean oil', 'whey', 'xanthan gum', 'polysorbate 60', 'guar gum'],\n",
    "        CANDY:['gelatin', 'carnauba wax', 'red 40', 'blue 1', 'yellow 5', 'citric acid', \n",
    "           'natural and artificial flavors', 'malic acid', 'sodium citrate'],\n",
    "        CHIPS_PRETZELS:['potatoes', 'onion powder', 'garlic powder', 'yeast extract', 'spices',\n",
    "                   'monosodium glutamate'],\n",
    "        CHOCOLATE:['cocoa butter', 'milk chocolate', 'chocolate', 'milk', 'dark chocolate', 'butter', 'chocolate liquor',\n",
    "              'skim milk', 'vanilla', 'palm oil'],\n",
    "        COOKIES:['baking soda', 'enriched flour', 'leavening', 'eggs', 'high fructose corn syrup', 'wheat flour', 'butter'],\n",
    "        POPCORN_PEANUTS:['almonds', 'peanuts', 'cashews', 'popcorn', 'pecans', 'peanut oil', 'raisins']\n",
    "    } \n",
    "    \n",
    "    num_rows = len(df)\n",
    "    data_dict = {key: set(values) for key, values in ingre_words.items()}\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in ingre_words.items() for word in words}\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in ingre_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['ingredients'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['ingredients'] = result_df.apply(lambda row: select_top_words(row, ingre_words, 'ingredients'), axis=1)\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3a2be",
   "metadata": {},
   "source": [
    "# 'household_serving_fulltext' column Research & Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cb942",
   "metadata": {},
   "source": [
    "After analyzing the results baised on the feature engineering train data set, we received the dict described above which includes indication words for every category.\n",
    "\n",
    "We will add a column for every selected word. it will contain 1 if the word is in appear in the snack household' column, and 0 else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6189ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_household(df):\n",
    "    household_words = {CAKES:['cake', 'cakes', 'cupcakes', 'cupcake','brownie', 'pie', 'donut', 'muffin', 'tart', \n",
    "                              'torte', 'doughnut','slice', 'pastry', 'bun', 'loaf'],\n",
    "                       CANDY:['candies', 'candy','gummies', 'gummy', 'marshmallow', 'pop', 'twist', 'stick','bear'],\n",
    "                       CHIPS_PRETZELS:['chip', 'chips', 'fries', 'crisp', 'chipsabout', 'pretzelsabout'],\n",
    "                       CHOCOLATE:['squares', 'square', 'balls', 'ball', 'truffle', 'praline', 'pralines', 'block', 'tablet', 'bar'],\n",
    "                       COOKIES:['cookies', 'cookie', 'wafers', 'wafer', 'crackers', 'cracker', 'biscuit', 'macaroon', 'waffle'],\n",
    "                       POPCORN_PEANUTS:['tbsp', 'kernel', 'popcorn', 'almond', 'shell', 'pecan']}\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    data_dict = {key: set(values) for key, values in household_words.items()}\n",
    "    columns_dict = {f\"{key}_{word}\": np.zeros(num_rows) for key, words in household_words.items() for word in words}\n",
    "    new_df = pd.DataFrame(columns_dict)\n",
    "    result_df = pd.concat([df.reset_index(drop=True), new_df], axis=1)\n",
    "    result_df.index = df.index\n",
    "\n",
    "    for key, words in household_words.items():\n",
    "        for word in words:\n",
    "            result_df[f\"{key}_{word}\"] = result_df['household_serving_fulltext'].apply(lambda x: 1 if isinstance(x, str) and word in x else 0)\n",
    "            \n",
    "    result_df['household_serving_fulltext'] = result_df.apply(lambda row: select_top_words(row, household_words, 'household_serving_fulltext'), axis=1)\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f5e7d",
   "metadata": {},
   "source": [
    "# 'serving_size' column Research & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d83b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Average Size</th>\n",
       "      <th>Median Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cookies_biscuits</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cakes_cupcakes_snack_cakes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chips_pretzels_snacks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>popcorn_peanuts_seeds_related_snacks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Category  Average Size  Median Size\n",
       "0                                 candy           NaN          NaN\n",
       "1                      cookies_biscuits           NaN          NaN\n",
       "2            cakes_cupcakes_snack_cakes           NaN          NaN\n",
       "3                 chips_pretzels_snacks           NaN          NaN\n",
       "4                             chocolate           NaN          NaN\n",
       "5  popcorn_peanuts_seeds_related_snacks           NaN          NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_by_category(df, category):\n",
    "    return df[df['category'] == category]\n",
    "\n",
    "def find_mean_and_median(category):\n",
    "    df = data_by_category(X_fe_w_cat, category)\n",
    "    return df['serving_size'].mean(), df['serving_size'].median()\n",
    "\n",
    "mean_median_df = {'Category': [CANDY, COOKIES, CAKES, CHIPS_PRETZELS, CHOCOLATE, POPCORN_PEANUTS],\n",
    "              'Average Size': [find_mean_and_median(CANDY)[0],find_mean_and_median(COOKIES)[0], find_mean_and_median(CAKES)[0],\n",
    "                               find_mean_and_median(CHIPS_PRETZELS)[0], find_mean_and_median(CHOCOLATE)[0],find_mean_and_median(POPCORN_PEANUTS)[0]],\n",
    "              'Median Size': [find_mean_and_median(CANDY)[1],find_mean_and_median(COOKIES)[1], find_mean_and_median(CAKES)[1],\n",
    "                               find_mean_and_median(CHIPS_PRETZELS)[1], find_mean_and_median(CHOCOLATE)[1],find_mean_and_median(POPCORN_PEANUTS)[1]]}\n",
    "size_df = pd.DataFrame(mean_median_df)\n",
    "\n",
    "size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686d72e",
   "metadata": {},
   "source": [
    "It seems that product from cakes category are much heavy then all the other products. Therefore, this column may help us distinguish between cakes category and all the others, so we'll keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc263bb7",
   "metadata": {},
   "source": [
    "# 'serving_size_unit' column Research & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c29f080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g</td>\n",
       "      <td>31743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8554</th>\n",
       "      <td>ml</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     values  Frequency\n",
       "0         g      31743\n",
       "8554     ml          8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_size_unit = {'values': data['serving_size_unit'].drop_duplicates(),\n",
    "                      'Frequency': [data['serving_size_unit'].value_counts()['g'], \n",
    "                                    data['serving_size_unit'].value_counts()['ml']]\n",
    "                    }\n",
    "serving_size_unit_df = pd.DataFrame(serving_size_unit)\n",
    "serving_size_unit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b03b75",
   "metadata": {},
   "source": [
    "We notice that most of the snacks has the value 'g' and just 8 of them has the value 'ml'.\n",
    "Therefore, \"size_unit\" column is not important and has no effect on the data, so it can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "606f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_size_unit_column(df):\n",
    "    return df.drop(['serving_size_unit'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac807e1",
   "metadata": {},
   "source": [
    "# Nutrients & Numerical columns Research & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a19e64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['serving_size', 'Calcium, Ca', 'Carbohydrate, by difference','Cholesterol', 'Energy', 'Fatty acids, total saturated',\n",
    "                    'Fatty acids, total trans', 'Fiber, total dietary', 'Iron, Fe','Protein', 'Sodium, Na', 'Sugars, total including NLEA',\n",
    "                    'Total lipid (fat)', 'Vitamin A, IU', 'Vitamin C, total ascorbic acid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ef1d0",
   "metadata": {},
   "source": [
    "- Dealing with missing values in the above columns - we will try different kinds of methods later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf17b7",
   "metadata": {},
   "source": [
    "# Part 3 - Models\n",
    "\n",
    "Notice that in all the next Cross Validation (CV) we will use simple XGBClassifier and we will fine-tuning it later.\n",
    "\n",
    "[Edit-Explain the modles that we are going to use]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77438670",
   "metadata": {},
   "source": [
    "## Feature Engineering - CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b1b12",
   "metadata": {},
   "source": [
    "[Model explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b87b2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_foo(df):\n",
    "    x1 = extract_brand(df)\n",
    "    x2 = extract_description(x1)\n",
    "    x3 = extract_ingredients(x2)\n",
    "    x4 = extract_household(x3)\n",
    "    return x4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cfb6c-fa58-4a55-9667-2cc105e97c3d",
   "metadata": {},
   "source": [
    "#### Proccess our fe-data and test if the dicts_words we built are good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f12adb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fe_train = final_foo(X_fe_train)\n",
    "X_fe_test = final_foo(X_fe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67c75a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = FunctionTransformer(extract_brand)\n",
    "Description2 = FunctionTransformer(extract_description)\n",
    "Ingredients2 = FunctionTransformer(extract_ingredients)\n",
    "Household2 = FunctionTransformer(extract_household)\n",
    "Drop = FunctionTransformer(drop_size_unit_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6c7c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [\"brand\", \"description\", \"ingredients\",\"household_serving_fulltext\",'serving_size_unit']\n",
    "\n",
    "numeric_features = ['serving_size', 'Calcium, Ca', 'Carbohydrate, by difference','Cholesterol', 'Energy', 'Fatty acids, total saturated',\n",
    "                    'Fatty acids, total trans', 'Fiber, total dietary', 'Iron, Fe','Protein', 'Sodium, Na', 'Sugars, total including NLEA',\n",
    "                    'Total lipid (fat)', 'Vitamin A, IU', 'Vitamin C, total ascorbic acid']\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4549ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea071e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.90569906 0.90733907 0.90648072 0.90812141 0.90689089]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, X_fe_train, y_fe_train.values.ravel(), cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac88e3",
   "metadata": {},
   "source": [
    "Accuracy ~0.9 that grate! Now, Let's fit on all X_fe_train and check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03a8579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                                  [&#x27;serving_size&#x27;,\n",
       "                                                   &#x27;Calcium, Ca&#x27;,\n",
       "                                                   &#x27;Carbohydrate, by &#x27;\n",
       "                                                   &#x27;difference&#x27;,\n",
       "                                                   &#x27;Cholesterol&#x27;, &#x27;Energy&#x27;,\n",
       "                                                   &#x27;Fatty acids, total &#x27;\n",
       "                                                   &#x27;saturated&#x27;,\n",
       "                                                   &#x27;Fatty acids, total trans&#x27;,\n",
       "                                                   &#x27;Fiber, total dietary&#x27;,\n",
       "                                                   &#x27;Iron, Fe&#x27;, &#x27;Protein&#x27;,\n",
       "                                                   &#x27;Sodium, Na&#x27;,\n",
       "                                                   &#x27;Sugars, total including &#x27;\n",
       "                                                   &#x27;NLEA&#x27;,\n",
       "                                                   &#x27;Total lipid (fat)&#x27;,\n",
       "                                                   &#x27;Vitamin A, IU&#x27;,\n",
       "                                                   &#x27;Vitamin C, t...\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                                  [&#x27;serving_size&#x27;,\n",
       "                                                   &#x27;Calcium, Ca&#x27;,\n",
       "                                                   &#x27;Carbohydrate, by &#x27;\n",
       "                                                   &#x27;difference&#x27;,\n",
       "                                                   &#x27;Cholesterol&#x27;, &#x27;Energy&#x27;,\n",
       "                                                   &#x27;Fatty acids, total &#x27;\n",
       "                                                   &#x27;saturated&#x27;,\n",
       "                                                   &#x27;Fatty acids, total trans&#x27;,\n",
       "                                                   &#x27;Fiber, total dietary&#x27;,\n",
       "                                                   &#x27;Iron, Fe&#x27;, &#x27;Protein&#x27;,\n",
       "                                                   &#x27;Sodium, Na&#x27;,\n",
       "                                                   &#x27;Sugars, total including &#x27;\n",
       "                                                   &#x27;NLEA&#x27;,\n",
       "                                                   &#x27;Total lipid (fat)&#x27;,\n",
       "                                                   &#x27;Vitamin A, IU&#x27;,\n",
       "                                                   &#x27;Vitamin C, t...\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               objective=&#x27;multi:softprob&#x27;, predictor=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                 [&#x27;serving_size&#x27;, &#x27;Calcium, Ca&#x27;,\n",
       "                                  &#x27;Carbohydrate, by difference&#x27;, &#x27;Cholesterol&#x27;,\n",
       "                                  &#x27;Energy&#x27;, &#x27;Fatty acids, total saturated&#x27;,\n",
       "                                  &#x27;Fatty acids, total trans&#x27;,\n",
       "                                  &#x27;Fiber, total dietary&#x27;, &#x27;Iron, Fe&#x27;, &#x27;Protein&#x27;,\n",
       "                                  &#x27;Sodium, Na&#x27;, &#x27;Sugars, total including NLEA&#x27;,\n",
       "                                  &#x27;Total lipid (fat)&#x27;, &#x27;Vitamin A, IU&#x27;,\n",
       "                                  &#x27;Vitamin C, total ascorbic acid&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False))]),\n",
       "                                 [&#x27;brand&#x27;, &#x27;description&#x27;, &#x27;ingredients&#x27;,\n",
       "                                  &#x27;household_serving_fulltext&#x27;,\n",
       "                                  &#x27;serving_size_unit&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;serving_size&#x27;, &#x27;Calcium, Ca&#x27;, &#x27;Carbohydrate, by difference&#x27;, &#x27;Cholesterol&#x27;, &#x27;Energy&#x27;, &#x27;Fatty acids, total saturated&#x27;, &#x27;Fatty acids, total trans&#x27;, &#x27;Fiber, total dietary&#x27;, &#x27;Iron, Fe&#x27;, &#x27;Protein&#x27;, &#x27;Sodium, Na&#x27;, &#x27;Sugars, total including NLEA&#x27;, &#x27;Total lipid (fat)&#x27;, &#x27;Vitamin A, IU&#x27;, &#x27;Vitamin C, total ascorbic acid&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;brand&#x27;, &#x27;description&#x27;, &#x27;ingredients&#x27;, &#x27;household_serving_fulltext&#x27;, &#x27;serving_size_unit&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
       "                                                  ['serving_size',\n",
       "                                                   'Calcium, Ca',\n",
       "                                                   'Carbohydrate, by '\n",
       "                                                   'difference',\n",
       "                                                   'Cholesterol', 'Energy',\n",
       "                                                   'Fatty acids, total '\n",
       "                                                   'saturated',\n",
       "                                                   'Fatty acids, total trans',\n",
       "                                                   'Fiber, total dietary',\n",
       "                                                   'Iron, Fe', 'Protein',\n",
       "                                                   'Sodium, Na',\n",
       "                                                   'Sugars, total including '\n",
       "                                                   'NLEA',\n",
       "                                                   'Total lipid (fat)',\n",
       "                                                   'Vitamin A, IU',\n",
       "                                                   'Vitamin C, t...\n",
       "                               grow_policy=None, importance_type=None,\n",
       "                               interaction_constraints=None, learning_rate=None,\n",
       "                               max_bin=None, max_cat_threshold=None,\n",
       "                               max_cat_to_onehot=None, max_delta_step=None,\n",
       "                               max_depth=None, max_leaves=None,\n",
       "                               min_child_weight=None, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=100,\n",
       "                               n_jobs=None, num_parallel_tree=None,\n",
       "                               objective='multi:softprob', predictor=None, ...))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_fe_train, y_fe_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75b5d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9124015748031497\n"
     ]
    }
   ],
   "source": [
    "test_score = pipeline.score(X_fe_test, y_fe_test.values.ravel())\n",
    "print(\"Test score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73349cd",
   "metadata": {},
   "source": [
    "## Fill Null values\n",
    "\n",
    "We want to make maximum use of the data and not throw away rows that contain a little missing information. (Remmember, we throw away rows which 80% of it data is missing). Therefore, in order to overcome this issue will check a few methods like mean, median and KNN, then will fill in the missing value with the best value that will help us.\n",
    "\n",
    "We built a pipleline that fill missing values in the numeric columns with different methods, after that it standrize these numeric columns and encode categorical columns.\n",
    "The categorical columns doesnt have missing values, therefore there isnt a imputer for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0edf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer()),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline_nan = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'preprocessor__num__imputer': [SimpleImputer(strategy='mean'), SimpleImputer(strategy='median')] + [KNNImputer(n_neighbors=k) for k in range(1, 6)]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_nan, param_grid, cv=5, n_jobs=1,verbose = 3,scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_fe_train, y_fe_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_imputer = grid_search.best_params_['preprocessor__num__imputer']\n",
    "print(\"Best Imputer:\", type(best_imputer).__name__)\n",
    "print(\"Best Parameters:\", best_imputer.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233bfd1",
   "metadata": {},
   "source": [
    "**Finally we got that KNN with 2_neigh method is the best way to fill in the missing value**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1a982",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "We will fill our null values with the KNN(2) <br>\n",
    "Since our data is imbalanced we will try apply SMOTE as we learn in class <br>\n",
    "We will check different parameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071bdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mt_train = final_foo(X_mt_train)\n",
    "X_mt_test = final_foo(X_mt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "X_mt_train[numeric_features] = imputer.fit_transform(X_mt_train[numeric_features])\n",
    "X_mt_test[numeric_features] = imputer.transform(X_mt_test[numeric_features])\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "663228b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    1957\n",
       "1    1941\n",
       "4    1353\n",
       "0     970\n",
       "3     966\n",
       "2     941\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see again that the data is imbalanced\n",
    "y_mt_train['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865980a2-d8d6-49a3-99de-ed8490722b60",
   "metadata": {},
   "source": [
    "We will preproccess the data like before, but before the classifier in the pipeline we will add a SMOTE object with different kind of parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d525058",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE()), \n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'resampler__sampling_strategy': ['auto', {5: 1957, 1: 1957, 4: 1957, 0: 1957, 3: 1957, 2: 1957}], # 1957 - the max count\n",
    "        'resampler__k_neighbors': [1, 3, 5, 7]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=1, verbose=3)\n",
    "grid_search.fit(X_mt_train, y_mt_train.values.ravel())\n",
    "\n",
    "y_pred = grid_search.predict(X_mt_test)\n",
    "print('Best Parameters:', grid_search.best_params_)\n",
    "print('Accuracy:', accuracy_score(y_mt_test, y_pred))\n",
    "print(classification_report(y_mt_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0c289",
   "metadata": {},
   "source": [
    "## Model Tuning - CV\n",
    "\n",
    "**Now will fine-tuning XGBC and 2 other popular classifiers: RF and GRADIENT_BOOST**\n",
    "We will do the model-tuning with the same method as before:\n",
    "- Building a pipeline with a specific model and we tune it using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcbee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = [\n",
    "#     {\n",
    "#         'classifier': XGBClassifier(random_state=42),\n",
    "#         'params': {\n",
    "#             'classifier__n_estimators': [50, 100, 200],\n",
    "#             'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'classifier__max_depth': [3, 5, 10],\n",
    "#             'classifier__subsample': [0.5, 0.8, 1],\n",
    "#             'classifier__colsample_bytree': [0.5, 0.8, 1]\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         'classifier': RandomForestClassifier(random_state=42),\n",
    "#         'params': {\n",
    "#             'classifier__n_estimators': [50, 100, 200],\n",
    "#             'classifier__max_depth': [None, 10, 20, 30],\n",
    "#             'classifier__min_samples_split': [2, 5, 10],\n",
    "#             'classifier__min_samples_leaf': [1, 2, 4]\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         'classifier': HistGradientBoostingClassifier(random_state=42),\n",
    "#         'params': {\n",
    "#             'classifier__max_iter': [50, 100, 200],\n",
    "#             'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'classifier__max_depth': [None, 10, 20, 30],\n",
    "#             'classifier__min_samples_leaf': [5, 10, 20]\n",
    "#         }\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# pipeline = ImbPipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('resampler', SMOTE(k_neighbors=3, sampling_strategy='auto')),\n",
    "#     ('classifier', None)\n",
    "# ])\n",
    "\n",
    "\n",
    "# for clf in classifiers:\n",
    "#     c = clf['classifier']\n",
    "#     pipeline.set_params(classifier=c)\n",
    "#     grid_search = GridSearchCV(pipeline, clf['params'], cv=5, n_jobs=1, verbose=3,scoring = 'accuracy')\n",
    "#     grid_search.fit(X_mt_train, y_mt_train)\n",
    "    \n",
    "#     print(\"Best parameters:\", grid_search.best_params_)\n",
    "#     print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "#     score = grid_search.score(X_mt_test, y_mt_test.values.ravel())\n",
    "#     print(\"Test set score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1698278",
   "metadata": {},
   "source": [
    "After LONG night of over 2000 fittings we got that the best parameters are:  \n",
    "**xgb**:  \n",
    "Best parameters: {'classifier__colsample_bytree': 0.5, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 10, 'classifier__n_estimators': 200, 'classifier__subsample': 1}  \n",
    "Best score: 0.9120326237108525  \n",
    "Test set score: 0.9232283464566929  \n",
    "\n",
    "**RF**:  \n",
    "Best parameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}  \n",
    "Best score: 0.9066192828082127  \n",
    "Test set score: 0.9148622047244095  \n",
    "\n",
    "**GB**:  \n",
    "Best parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': None, 'classifier__max_iter': 200, 'classifier__min_samples_leaf': 20}  \n",
    "Best score: 0.9130167092440156  \n",
    "Test set score: 0.9178149606299213  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f92aef-8233-4e03-994a-00ba5810d111",
   "metadata": {},
   "source": [
    "# Let's summarize what we got\n",
    "### filling nan values with KNN(n_neigh = 2)\n",
    "### resampling with SMOTE(k_neighbors=3, sampling_strategy='auto')\n",
    "#### XGB - {'classifier__colsample_bytree': 0.5, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 10, 'classifier__n_estimators': 200, 'classifier__subsample': 1}\n",
    "#### RF - {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
    "#### GB - {'classifier__learning_rate': 0.1, 'classifier__max_depth': None, 'classifier__max_iter': 200, 'classifier__min_samples_leaf': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78900f58-ccf1-4315-ac01-2f3017b10b8a",
   "metadata": {},
   "source": [
    "# Now for the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4fea5-fd52-448e-b7ac-4420511bc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = final_foo(X_train)\n",
    "X_test = final_foo(X_test)\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "X_train[numeric_features] = imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = imputer.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db1d9f-b538-43a8-bda1-51fe6f67682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    {\n",
    "        'classifier': XGBClassifier(colsample_bytree=0.5,learning_rate=0.2,max_depth=10,n_estimators=200,subsample=1)\n",
    "    },\n",
    "    {\n",
    "        'classifier': RandomForestClassifier(max_depth=None,min_samples_leaf=1,min_samples_split=2,n_estimators=200)\n",
    "    },\n",
    "    {\n",
    "        'classifier': HistGradientBoostingClassifier(learning_rate=0.1,max_depth=None,max_iter=200,min_samples_leaf=20)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62037714-b4fa-4404-a407-7b21b4f9222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE(k_neighbors=3, sampling_strategy='auto')),  # Resampler included directly in the pipeline\n",
    "    ('classifier', None)\n",
    "])\n",
    "\n",
    "\n",
    "for clf in classifiers:\n",
    "    c = clf['classifier']\n",
    "    pipeline.set_params(classifier=c)\n",
    "    print(f\"fitting {c}...\")\n",
    "    pipeline.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    test_score = pipeline.score(X_test, y_test.values.ravel())\n",
    "    print(f\"Test score for {c} on df1:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d17a6-1f5a-4a6a-951c-bbe7b7be99e8",
   "metadata": {},
   "source": [
    "### Nice results!\n",
    "### Remember df2? Lets try our final pipeline on it and decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db591c8f-7f00-4ebb-9b25-d066cdb7431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = df2.loc[:,df2.columns != 'category']\n",
    "y_p = df2.loc[:,['category']]\n",
    "\n",
    "y_p['category'].replace(['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate',\n",
    " 'cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                        [0, 1, 2, 3, 4, 5], inplace=True)\n",
    "\n",
    "X_p_train, X_p_test, y_p_train, y_p_test = train_test_split(X_p, y_p, test_size=0.2, random_state=42, stratify=y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13438ca3-1f33-43b7-94fb-86ef4bbd8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p_train = final_foo(X_p_train)\n",
    "X_p_test = final_foo(X_p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e4ef7-cc4d-44c0-a9fb-72d376d72944",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "X_p_train[numeric_features] = imputer.fit_transform(X_p_train[numeric_features])\n",
    "X_p_test[numeric_features] = imputer.transform(X_p_test[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b32dab-75e2-4160-babf-98e156a7a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    c = clf['classifier']\n",
    "    pipeline.set_params(classifier=c)\n",
    "    print(f\"fitting {c}...\")\n",
    "    pipeline.fit(X_p_train, y_p_train.values.ravel())\n",
    "\n",
    "    test_score = pipeline.score(X_p_test, y_p_test.values.ravel())\n",
    "    print(f\"Test score for {c} on df2:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48605c-61d7-4566-9399-4e330fb7be28",
   "metadata": {},
   "source": [
    "# WHOOOO WE R DONE!\n",
    "## Our 3 final models are:\n",
    "### ..... on df1\n",
    "### ..... on df1\n",
    "### .....on df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e40dc7-8a61-4636-9a92-ae3b8cd42cb8",
   "metadata": {},
   "source": [
    "#### We know the following code is not efficient and repeats some steps BUT we rather be safe than sorry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b6e9d-84b9-400f-8736-4febfe5a95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_foo(X)\n",
    "X_p = final_foo(X_p)\n",
    "data_test = final_foo(data_test)\n",
    "data_with_probs_test = final_foo(data_with_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b0e73-7e98-4e27-a0dd-7acfbd2f91cd",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ec4bd-e80f-44b7-b66a-fbbf8c8d7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.copy()\n",
    "data_test1 = data_test.copy()\n",
    "y1 = y.copy()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "X1[numeric_features] = imputer.fit_transform(X1[numeric_features])\n",
    "data_test1[numeric_features] = imputer.transform(data_test1[numeric_features])\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE(k_neighbors=3, sampling_strategy='auto')),\n",
    "    ('classifier', RandomForestClassifier(max_depth=None,min_samples_leaf=1,min_samples_split=2,n_estimators=200))\n",
    "])\n",
    "\n",
    "pipeline.fit(X1, y1.values.ravel())\n",
    "\n",
    "data_test1['pred_cat'] = pipeline.predict(data_test1)\n",
    "\n",
    "data_test1['pred_cat'].replace([0, 1, 2, 3, 4, 5],\n",
    "                              ['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate','cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                              inplace=True)\n",
    "\n",
    "data_test1.loc[:,['idx','pred_cat']]#.to_csv('model01.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7487a-4177-411b-a204-e9ca94d76f84",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221e50b-871d-4331-bc33-ebf668e7d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X_p.copy()\n",
    "data_test2 = data_test.copy()\n",
    "y2 = y_p.copy()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "X2[numeric_features] = imputer.fit_transform(X2[numeric_features])\n",
    "data_test2[numeric_features] = imputer.transform(data_test2[numeric_features])\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE(k_neighbors=3, sampling_strategy='auto')),\n",
    "    ('classifier', RandomForestClassifier(max_depth=None,min_samples_leaf=1,min_samples_split=2,n_estimators=200))\n",
    "])\n",
    "\n",
    "pipeline.fit(X2, y2.values.ravel())\n",
    "\n",
    "data_test2['pred_cat'] = pipeline.predict(data_test2)\n",
    "\n",
    "data_test2['pred_cat'].replace([0, 1, 2, 3, 4, 5],\n",
    "                              ['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate','cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                              inplace=True)\n",
    "\n",
    "data_test2.loc[:,['idx','pred_cat']]#.to_csv('model02.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710e6a2-8bb3-432d-90f7-57f230cf234b",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15ac1f-2e84-424d-9164-655f58760fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = X.copy()\n",
    "data_test3 = data_test3.copy()\n",
    "y3 = y.copy()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "X3[numeric_features] = imputer.fit_transform(X3[numeric_features])\n",
    "data_test3[numeric_features] = imputer.transform(data_test3[numeric_features])\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE(k_neighbors=3, sampling_strategy='auto')),\n",
    "    ('classifier', RandomForestClassifier(max_depth=None,min_samples_leaf=1,min_samples_split=2,n_estimators=200))\n",
    "])\n",
    "\n",
    "pipeline.fit(X3, y3.values.ravel())\n",
    "\n",
    "data_test3['pred_cat'] = pipeline.predict(data_test3)\n",
    "\n",
    "data_test3['pred_cat'].replace([0, 1, 2, 3, 4, 5],\n",
    "                              ['cakes_cupcakes_snack_cakes', 'candy', 'chips_pretzels_snacks', 'chocolate','cookies_biscuits', 'popcorn_peanuts_seeds_related_snacks'],\n",
    "                              inplace=True)\n",
    "\n",
    "data_test3.loc[:,['idx','pred_cat']]#.to_csv('model03.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
